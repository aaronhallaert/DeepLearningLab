{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jYysdyb-CaWM"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "# Getting started with machine learning and deep learning on Google Colab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pWu8tW5KjmeU"
   },
   "source": [
    "## 0. setting up for Tensorflow\n",
    "\n",
    "You have successfully opened this notebook in Colab. The nice thing about Colab is that most things are already installed. However, there are a few things you still need to do. The first is to make sure you are using the right version of Tensorflow: you need 2.0 but at the moment of writing this notebook, this is not the default version yet. In order to fix this, run the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "T_OYk0TukLkJ"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "  # %tensorflow_version only exists in Colab.\n",
    "  %tensorflow_version 2.x\n",
    "except Exception:\n",
    "  pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QXuJfDQkkZAK"
   },
   "source": [
    "The second thing is doing some standard (and some less standard) imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "hkn56XM-kfWW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "# TensorFlow and tf.keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Helper libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn as sk\n",
    "\n",
    "print(tf.__version__)\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 2020\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dORASwMscEOv"
   },
   "source": [
    "## 1. Steps in machine learning\n",
    "\n",
    "In class we identified the following important steps in machine learning:\n",
    "\n",
    "1. Problem and data analysis\n",
    "2. Train/validate/test split\n",
    "3. Preprocessing and feature extraction\n",
    "4. Model training and hyperparameter tuning\n",
    "5. Analysis of performance and errors\n",
    "\n",
    "Steps 1 and 3 are usually very specific to the problem and the dataset, so we will not address them here. What we will provide are examples for the most common operations in steps 2, 4 and 5 when working with Tensorflow-Keras. Note that it is common to Python that there are usually many ways in which to do things. From that perspective, what we show below is only one example and you may find others online.\n",
    "\n",
    "## 2. Setting up and splitting the data\n",
    "\n",
    "The first thing you need to do in any supervised machine learning problem is importing the data. As is always the case in Python, there are many ways to to this, which depend on the specific data you are using. For educational purposes, some simple datasets are provided with either Scikit-Learn (sklearn - https://scikit-learn.org/stable/) or Tensorflow. For these datasets, there are built-in functions to import them. Many other datasets are provided as .csv files, which are most easily imported with Pandas (https://pandas.pydata.org). For more complex datasets, you may possibly have to write your own importing functions.\n",
    "\n",
    "Once the data is imported, you can start analysing it, which includes visualising and extracting statistics. This should give you insight into good preprocessing, data cleaning and feature extraction steps, eventually resulting in the features you will use in your model.\n",
    "\n",
    "In this example we will use two built-in datasets, one for regression and one for classification. We start with the regression data set and address specific changes for classification data sets at the end of this notebook.\n",
    "\n",
    "The data set is the Boston house prices data set that was also discussed in class. You can load it as follows:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XqFTVU-4jCds"
   },
   "outputs": [],
   "source": [
    "boston_housing = keras.datasets.boston_housing\n",
    "\n",
    "(all_train_data, all_train_labels), (X_test, r_test) = boston_housing.load_data()\n",
    "\n",
    "# Shuffle the training set\n",
    "order = np.argsort(np.random.random(all_train_labels.shape))\n",
    "all_train_data = all_train_data[order]\n",
    "all_train_labels = all_train_labels[order]\n",
    "\n",
    "print(\"Training set: {}\".format(all_train_data.shape))  # 404 examples, 13 features\n",
    "print(\"Test set:  {}\".format(X_test.shape))   # 102 examples, 13 features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ejf2HSCA8MSY"
   },
   "source": [
    "The data is stored with a fixed test set, so we do not need to split this off. We do, however, need to split off at least one validation set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5aOwXuIr8Mz9"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "val_fraction = 0.25\n",
    "X_train, X_val, r_train, r_val = train_test_split(all_train_data, all_train_labels, \n",
    "                                                    test_size = val_fraction, \n",
    "                                                    random_state=0)\n",
    "\n",
    "print(\"Training set: {}\".format(X_train.shape)) \n",
    "print(\"Validation set:  {}\".format(X_val.shape))   \n",
    "print(\"Test set:  {}\".format(X_test.shape))   \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eum5QXfF7zaq"
   },
   "source": [
    "The dataset contains 13 different features:\n",
    "\n",
    "1.   Per capita crime rate.\n",
    "2.   The proportion of residential land zoned for lots over 25,000 square feet.\n",
    "3.   The proportion of non-retail business acres per town.\n",
    "4.   Charles River dummy variable (= 1 if tract bounds river; 0 otherwise).\n",
    "5.   Nitric oxides concentration (parts per 10 million).\n",
    "6.   The average number of rooms per dwelling.\n",
    "7.   The proportion of owner-occupied units built before 1940.\n",
    "8.   Weighted distances to five Boston employment centers.\n",
    "9.   Index of accessibility to radial highways.\n",
    "10.  Full-value property-tax rate per $10,000.\n",
    "11.  Pupil-teacher ratio by town.\n",
    "12.  1000 * (Bk - 0.63) ** 2 where Bk is the proportion of Black people by town.\n",
    "13.  Percentage lower status of the population.\n",
    "\n",
    "Each one of these input data features is stored using a different scale. Some features are represented by a proportion between 0 and 1, other features are ranges between 1 and 12, some are ranges between 0 and 100, and so on. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VWVKE_SwClhB"
   },
   "outputs": [],
   "source": [
    "print(X_train[0])  # First training sample, not normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d9uH1a63CmiO"
   },
   "source": [
    "As seen in class, it is advised to rescale these data first, such that they are all in the same range. The easiest way to do this is to use one of sklearn's 'scalers'. In this case, we will use the \"StandardScaler\". This \"normalises each feature individually by subtracting the mean and then dividing by the standard deviation. The parameters of the scaler, the mean to subtract and the stdev to divide by are 'fitted' first. In order to avoid information leakage, this fitting should happen ONLY on the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v4ubKgHK-Ks_"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "# the scaler parameters are \"fitted\" on the training data\n",
    "# (i.e.: calculate each feature's average and standard deviation in the training set)\n",
    "scaler.fit(X_train)\n",
    "\n",
    "# now use the \"fitted\" (learned) scaler parameters to transform the training data\n",
    "X_train_s = scaler.transform(X_train)\n",
    "\n",
    "\n",
    "# it is crucial to do the same transformations to the validation and test data\n",
    "# (so the SAME avg and stdev are used for all three data sets)\n",
    "X_val_s = scaler.transform(X_val)\n",
    "X_test_s = scaler.transform(X_test)\n",
    "\n",
    "print(X_train_s[0])  # First training sample,  rescaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jYN2IXAL8IYG"
   },
   "source": [
    "We see that now, all features more or less have the same scale and they are centered around zero. For now, this is all the preprocessing we will do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jr5dDNmVBG43"
   },
   "source": [
    "## 2. Training a linear model\n",
    "\n",
    "We haven't seen neural networks in depth yet. However, it was mentioned that all but the last layers of a neural network can also be considerered as a trained feature extractor. The features it extracts are the inputs to the last layer, which is nothing more than a linear model (regression or classification). For now, we will therefore omit the feature extraction layers and \"abuse\" Keras to train a linear model on top of the features in our data set.\n",
    "\n",
    "The way Keras works is as follows:\n",
    "\n",
    "1. Define the structure of your model\n",
    "2. Compile it: this builds the computational graph to be used when calculating the model outputs (inference) as well as the one for gradient descent\n",
    "3. Fit the model to the training data\n",
    "4. Evaluate the model\n",
    "\n",
    "Since we will be using linear regression, the model structure is fixed. It consists of one layer which connects all input features to the output. In Keras, such a layer is called a \"Dense\" layer. We will put the code for constructing the model and compiling it in a function wrapper. This way you can easily re-use it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "-NCsvKSOHZfP"
   },
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "def build_model(inp_shape, lr = 1.0,L2=0.0):\n",
    "  # a model is always built up as a list of layers\n",
    "  # In the first layer, you need to define the shape of the input\n",
    "  # all other layers automatically use the shape of the previous layer's output\n",
    "  # a linear model only has a single layer\n",
    "\n",
    "  model = keras.Sequential([\n",
    "    keras.layers.Dense(1, input_shape=(inp_shape,),\n",
    "                       kernel_regularizer=regularizers.l2(L2))\n",
    "  ])\n",
    "\n",
    "  # When compiling the model, you have to tell it which training algorithm we will be using\n",
    "  # Details about these are not relevant at this point \n",
    "  # (we will use SGD, which is what we called \"batch gradient descent in class\")\n",
    "  # Do note, however, that the learning rate is a parameter for the optimiser\n",
    "  optimizer = tf.keras.optimizers.SGD(learning_rate=lr)\n",
    "\n",
    "  # You also need to define which loss function we will be optimising \n",
    "  # and which alternative loss functions to keep track of\n",
    "\n",
    "  # In this case, we optimise MSE, but also keep track of the mean absolute error (MAE)\n",
    "  model.compile(loss='mse',\n",
    "                optimizer=optimizer,\n",
    "                metrics=['mae'])\n",
    "  return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FbVhjPpzn6BM"
   },
   "source": [
    "The summary gives an overview of all layers in the model. It also shows the output shape of each layer and the number of trainable parameters it has. In this case, since we are doing linear regression with 13 features, the model has 14 parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jL3OqFKZ9dFg"
   },
   "outputs": [],
   "source": [
    "model = build_model(X_train_s.shape[1], lr = 0.2) # call model for right number of features\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2TN6n3kKn_iy"
   },
   "source": [
    "\n",
    "Next, we build and train the model:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "IpzxnXyzoPMn"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 5\n",
    "\n",
    "# Store training stats per EPOCH in history to allow plotting afterwards\n",
    "history = model.fit(X_train_s, r_train, batch_size=10, epochs=EPOCHS,\n",
    "                    validation_data=(X_val_s,r_val), verbose=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R9M-_4qSoU-a"
   },
   "source": [
    "**Very important warning:** Always re-build your model if you want it to train with different settings (e.g. batch size). Otherwise, it will continue training where it left off. For this reason, it is usually safest to put the code that creates and compiles the model in the same code block (or function wrapper) as the actual training.\n",
    "\n",
    "\n",
    "The code fragment below defines a function you can use to plot the training history and then uses it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zyr22w1rLM9B"
   },
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "  plt.figure(figsize = (12,4))\n",
    "  plt.subplot(1,2,1)\n",
    "\n",
    "  plt.xlabel('Epoch')\n",
    "  plt.ylabel('Mean squared error')\n",
    "  plt.plot(history.epoch, np.array(history.history['loss']), \n",
    "           label='Train MSE')\n",
    "  plt.plot(history.epoch, np.array(history.history['val_loss']),\n",
    "           label = 'Val MSE')\n",
    "  plt.legend()\n",
    "\n",
    "  plt.subplot(1,2,2)\n",
    "  plt.xlabel('Epoch')\n",
    "  plt.ylabel('Mean Absolute Error')\n",
    "  plt.plot(history.epoch, np.array(history.history['mae']), \n",
    "           label='Train MAE')\n",
    "  plt.plot(history.epoch, np.array(history.history['val_mae']),\n",
    "           label = 'Val MAE')\n",
    "  plt.legend()\n",
    "  \n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VcTT2MVnMgHO"
   },
   "source": [
    "What you want to achieve in gradient descent is that the loss curves for training and validation data smoothly go down. We say that learning has **converged** when the training loss no longer decreases. For this simple model, there can barely be overfitting. This means that we expect the validation loss to decrease along with the training loss. If a model **can** overfit, then the validation loss will start to increase again at some point. Since the validation loss is our estimate of \"out-of-sample\" performance, we should stop learning at the point where it is minimal. \n",
    "\n",
    "In the plot above, loss values \"explode\". This indicates that you have entered the regime where your gradients get larger and larger and you jump further and further from the minimum. In that case you definitely need to reduce the learning rate!\n",
    "\n",
    "**Task:**\n",
    "Now try to adapt the learning rate, the batch size and the number of epochs until you see a **learning convergence: nice curves that smoothly decrease and then remain stable for a few epochs at minimal loss. In addition, try to achieve this with as few epochs as you can. \n",
    "\n",
    "\n",
    "\n",
    "**Note:** Normally, you would expect the final training loss to be smaller than the validation loss. However, the train-validate split is random. Although the validation set is quite large, in proportion to the training set, it *may* still occur that it contains relatively more \"hard\" examples than the training set. Because there is hardly any overfitting, this *may* someties give you the opposite (i.e. val loss lower than train loss). From understanding the theory, you should know that this is not what you would expect. \n",
    "\n",
    "Ideally, if you see this: construct a different split (in this case by re-running the code). \n",
    "\n",
    "**Because of this variability, it is therefore good practice to set a random seed before the splitting OR (even better) to store your train-validate split (e.g., as a vector) to ensure you train and validate all models with the exact same split!**\n",
    "\n",
    "## 3. Model evaluation and data splitting issues\n",
    "\n",
    "Now that the model is trained, we need to evalate it. We will first get the model outputs for train, validate and test sets, as well as all corresponding scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kWOv0N1GTBBg"
   },
   "outputs": [],
   "source": [
    "[train_loss, train_mae] = model.evaluate(X_train_s, r_train, verbose=0)\n",
    "print(\"Training set Mean Squared Error:{:7.2f}\".format(train_loss))\n",
    "print(\"Training set Mean Absolute Error:{:7.2f}\\n\".format(train_mae))\n",
    "\n",
    "[val_loss, val_mae] = model.evaluate(X_val_s, r_val, verbose=0)\n",
    "print(\"Validation set Mean Squared Error:{:7.2f}\".format(val_loss))\n",
    "print(\"Validation set Mean Absolute Error:{:7.2f}\\n\".format(val_mae))\n",
    "\n",
    "[test_loss, test_mae] = model.evaluate(X_test_s, r_test, verbose=0)\n",
    "print(\"Test set Mean Squared Error:{:7.2f}\".format(test_loss))\n",
    "print(\"Test set Mean Absolute Error:{:7.2f}\\n\".format(test_mae))\n",
    "\n",
    "# We'll make a nice plot to visualise this\n",
    "# Note that this can be done a lot more elegantly, e.g. with Seaborn\n",
    "\n",
    "train_values = [train_loss, train_mae]\n",
    "val_values = [val_loss, val_mae]\n",
    "test_values = [test_loss, test_mae]\n",
    "labels = ['MSE', 'MAE']\n",
    "\n",
    "\n",
    "x=np.arange(2)\n",
    "width = 0.2  # the width of the bars\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "rects1 = ax.bar(x - width, train_values, width, label='Train')\n",
    "rects2 = ax.bar(x , val_values, width, label='Validate')\n",
    "rects3 = ax.bar(x + width, test_values, width, label='Test')\n",
    "\n",
    "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "ax.set_ylabel('Scores')\n",
    "#ax.set_title('Scores by group and gender')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(labels)\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wZ14iV2PwsUu"
   },
   "source": [
    "**A side note on how to iteratively improve your model:**\n",
    "\n",
    "At this point, as a machine learner, you should have a clear expectation of what to expect. In this case, validation and test sets are approximately the same size, and the validation set has not been used yet to make any decisions about the model. In principle, the model should perform equally well on both. If the validation set has been used, test loss is usually a bit (but not much) worse than validation loss. Any deviations from this rule, in particularly unusually large gaps, can usually be related to problems with the data, the splitting of the data, or information leakage. \n",
    "\n",
    "**Checking whether your data splitting is reliable and whether your code does not contain information leakage is one of the only occasions where it is allowed to look at test scores during model optimisation.**\n",
    "\n",
    "In this case, we observe something unexpected: the validation MSE is considerably worse than the test mse. The only reason for this can be that **the i.i.d. assumption is violated**. We can only assume that the train-test split was done without information leakage (it was done for us). As far as we know, there was no information leakage in the train/validate split, but we may not have received all relevant information: there could be correlations in the data that we don't know about! This only leaves the assumption of the data being identically distributed. This could be due to the random splitting, leading to different feature or label distributions (can be checked by plotting histograms) or it could be caused by a small number of outliers in the train or validation set. At this point, as a machine learning, you should start analysing the data to try and identify which of the causes mentioned above applies to your data.\n",
    "\n",
    "A second observation is that the difference between both sets is much smaller for MAE then for MSE. This refines our hypothesis of where the problem lies. MSE takes the square of the errors, which means that large errors are \"blown up\". We can assume that a few examples, possibly outliers or mislabeled samples, occur in the validation set but not in the test set.\n",
    "\n",
    "At this point, having phrased a hypothesis of what is wrong, a machine learner should go back to the data: find the samples where your model makes the largest errors, decide if they are real outliers, or maybe mislabeled samples, and decide between trying to correct them or removing them from the data set. Then you validate your hypothesis by re-running the model and (hopefully) concluding that the problem is solved. If you do not solve this, then any more complex model with lots of hyperparameters will be tuned on a validation set that is not reliable.\n",
    "\n",
    "Feel free to try and fix this problem. Possible actions are:\n",
    "- identify the validation and test samples with the largest errors\n",
    "- plot histograms of the features and the labels in validation and test sets and compare them\n",
    "- decide whether either the features or the labels of the worst validation examples can be considered as outliers\n",
    "- If there are really few outliers: remove them and see if the results become more stable\n",
    "\n",
    "However, since preprocessing is not the core topic of this tutorial, we will switch to the main topic again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KiC7CWhkIEo-"
   },
   "source": [
    "## 4. Iterative model improvement\n",
    "\n",
    "Since this tutorial is restricted to linear models, and we don't have enough background on the data to do proper domain-specific feature expansion, there is not a lot we can do. For this reason, we will only explore polynomial feature expansion here. Sklearn provides a function to do this:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AAS_f9TWX9ww"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# fitting this module only involves counting the number of features\n",
    "\n",
    "polytf2 = PolynomialFeatures(degree=3,include_bias=False)\n",
    "\n",
    "polytf2.fit(X_train)\n",
    "\n",
    "X_train_p2 = polytf2.transform(X_train)\n",
    "X_val_p2 = polytf2.transform(X_val)\n",
    "X_test_p2 = polytf2.transform(X_test)\n",
    "\n",
    "print(X_train_p2.shape)\n",
    "print(X_val_p2.shape)\n",
    "print(X_test_p2.shape)\n",
    "\n",
    "# Before using them, we need to scale the transformed features\n",
    "scaler2 = StandardScaler()\n",
    "scaler2.fit(X_train_p2)\n",
    "\n",
    "X_train_p2s = scaler2.transform(X_train_p2)\n",
    "X_val_p2s = scaler2.transform(X_val_p2)\n",
    "X_test_p2s = scaler2.transform(X_test_p2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aiHUTDftY_mn"
   },
   "source": [
    "As you can see, we have now expanded the original feature set to 104 new features. These include the 13 original features, the 13 squares of these, and the 78 unique cross products between original features. We will now again train a linear model and see what comes out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HZgMbxI2Zt0n"
   },
   "outputs": [],
   "source": [
    "# call model for right number of features\n",
    "# in addition, we need to decrease the learning rate in this case\n",
    "# check for yourself what happens for larger learning rates\n",
    "model_p2 = build_model(X_train_p2.shape[1], lr = 0.5)\n",
    "model_p2.summary()\n",
    "\n",
    "EPOCHS = 5\n",
    "\n",
    "# Store training stats per EPOCH in history to allow plotting afterwards\n",
    "history_p2 = model_p2.fit(X_train_p2s, r_train, epochs=EPOCHS, batch_size = 10,\n",
    "                    validation_data=(X_val_p2s,r_val), verbose=0)\n",
    "plot_history(history_p2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lxsmFulNe1-C"
   },
   "outputs": [],
   "source": [
    "[train_loss_p2, train_mae_p2] = model_p2.evaluate(X_train_p2s, r_train, verbose=0)\n",
    "print(\"Training set Mean Squared Error:{:7.2f}\".format(train_loss_p2))\n",
    "print(\"Training set Mean Absolute Error:{:7.2f}\\n\".format(train_mae_p2))\n",
    "\n",
    "[val_loss_p2, val_mae_p2] = model_p2.evaluate(X_val_p2s, r_val, verbose=0)\n",
    "print(\"Validation set Mean Squared Error:{:7.2f}\".format(val_loss_p2))\n",
    "print(\"Validation set Mean Absolute Error:{:7.2f}\\n\".format(val_mae_p2))\n",
    "\n",
    "[test_loss_p2, test_mae_p2] = model_p2.evaluate(X_test_p2s, r_test, verbose=0)\n",
    "print(\"Test set Mean Squared Error:{:7.2f}\".format(test_loss_p2))\n",
    "print(\"Test set Mean Absolute Error:{:7.2f}\\n\".format(test_mae_p2))\n",
    "\n",
    "# We'll make a nice plot to visualise this\n",
    "# Note that this can be done a lot more elegantly, e.g. with Seaborn\n",
    "\n",
    "train_values = [train_loss_p2, train_mae_p2]\n",
    "val_values = [val_loss_p2, val_mae_p2]\n",
    "test_values = [test_loss_p2, test_mae_p2]\n",
    "labels = ['MSE', 'MAE']\n",
    "\n",
    "\n",
    "x=np.arange(2)\n",
    "width = 0.2  # the width of the bars\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "rects1 = ax.bar(x - width, train_values, width, label='Train')\n",
    "rects2 = ax.bar(x , val_values, width, label='Validate')\n",
    "rects3 = ax.bar(x + width, test_values, width, label='Test')\n",
    "\n",
    "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "ax.set_ylabel('Scores')\n",
    "#ax.set_title('Scores by group and gender')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(labels)\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PPDOyMAD2CMt"
   },
   "source": [
    "**Task:**\n",
    "Just like before: tune learning rate, batch size and epochs to get convergence.\n",
    "\n",
    "Once that is done, you will see that, although the train and validation losses have improved with more features, this is not the case for the test loss. Overfitting has increased, so the next step is to regularise. The easiest way to do this is to use L2 regularisation (also called *ridge regression* for linear regression models). This introduces a hyperparameter that needs to be optimised, based on the validation score. This is usually done using *gridsearch*, i.e. by training and validating the network for a range of values. The L2 and L1 regularisation parameters are usually swept on a logarithmic scale, i.e., you will try value ranges like [1.0e-2, 1.0e-1, 1.0, 10.0, 100.0]. However, you should always check the range you are sweeping: \n",
    "- do I find a minimum loss inside that region or do I need to extend / shift my range? \n",
    "- is my minimum not too sharp? If so, I need to refine my sweep.\n",
    "\n",
    "What you typically do is:\n",
    "- fix learning convergence without extensive regularisation, keep on monitoring learning convergence throughout the next steps\n",
    "- try to add regularisation and increase it until it has an effect (when that happens depends on the task and the data)\n",
    "- increase regularisation until the result gets worse again\n",
    "\n",
    "This way you can iteratively try to find a reasonable balance between overfitting and too much regularisation. The build_model function has a regularisation parameter as an input, so you can try this out yourself. However, note that the train and validation curves in the plot may become less smooth. If that happens, you need to decrease the learning rate and possibly also run for more epochs. \n",
    "\n",
    "This is what occurs in the example below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yPa85o6861N7"
   },
   "outputs": [],
   "source": [
    "# call model for right number of features\n",
    "# in addition, we need to decrease the learning rate in this case\n",
    "# check for yourself what happens for larger learning rates\n",
    "model_p2r = build_model(X_train_p2s.shape[1],lr=0.2, L2 = 0.01)\n",
    "model_p2r.summary()\n",
    "\n",
    "EPOCHS = 5\n",
    "\n",
    "# Store training stats per EPOCH in history to allow plotting afterwards\n",
    "history_p2r = model_p2r.fit(X_train_p2s, r_train, epochs=EPOCHS,batch_size = 10,\n",
    "                    validation_data=(X_val_p2s,r_val), verbose=0)\n",
    "plot_history(history_p2r)\n",
    "\n",
    "[train_loss_p2r, train_mae_p2r] = model_p2r.evaluate(X_train_p2s, r_train, verbose=0)\n",
    "print(\"Training set Mean Squared Error:{:7.2f}\".format(train_loss_p2r))\n",
    "print(\"Training set Mean Absolute Error:{:7.2f}\\n\".format(train_mae_p2r))\n",
    "\n",
    "[val_loss_p2r, val_mae_p2r] = model_p2r.evaluate(X_val_p2s, r_val, verbose=0)\n",
    "print(\"Validation set Mean Squared Error:{:7.2f}\".format(val_loss_p2r))\n",
    "print(\"Validation set Mean Absolute Error:{:7.2f}\\n\".format(val_mae_p2r))\n",
    "\n",
    "[test_loss_p2r, test_mae_p2r] = model_p2r.evaluate(X_test_p2s, r_test, verbose=0)\n",
    "print(\"Test set Mean Squared Error:{:7.2f}\".format(test_loss_p2r))\n",
    "print(\"Test set Mean Absolute Error:{:7.2f}\\n\".format(test_mae_p2r))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h2htOmU8-Aeo"
   },
   "source": [
    "You will learn thoughout this course that monitoring train and validation scores is crucial for achieving good results with deep learning. Achieving curves that are quite smooth and have converged far enough should always be your priority before changing anything else. **You can not compare the results for networks that have not properly converged!!** This is done mostly by adapting the learning rate, the batch size and the number of EPOCHS. These parameters can not be tuned once and then kept fixed, since their optimal values depend on many other factors in the network. We will treat this in depth in this course.\n",
    "\n",
    "**Task:** try to achieve a better score with polynomial regression:\n",
    "- tune the amount of L2 regularisation, the learning rate and the number of epochs for the second degree polynomials\n",
    "- If that works out, try if you can do better with a third or higher degree polynomial\n",
    "\n",
    "Don't expect miracles, though: this data set is very noisy!\n",
    "\n",
    "**Additional info for sklearn users**\n",
    "You can also use Tensorflow models in sklearn (e.g. in a pipeline) by putting it inside a function wrapper that can be used as an sklearn model. You can read more about this [here](https://www.tensorflow.org/api_docs/python/tf/keras/wrappers/scikit_learn). This also allows you to use keras models in sklearn pipelines. \n",
    "**However:** using the blind grid searches that are common in sklearn is not advised in deep learning, because they no longer allwe you to monitor what happens (such as problems with learning convergence)! \n",
    "\n",
    "Also, for deep learning, training quickly becomes compute-intensive, so doing full grid-searches is less common, unless you have huge computational resources at your disposal. \n",
    "\n",
    "**Just for the fun of it**\n",
    "Since you already know that neural networks are essentially stacks of layers, we will show below how to make a deeper model for the original features.\n",
    "\n",
    "In the code, a single hidden layer is added. This introduces an additional hyperparameter: the number of neurons on this hidden layer. Check out how this affects the total number of weights by looking at the model summary().\n",
    "\n",
    "Now play with this model to try and get an even better score. You can make it wider (more neurons per hidden layer) and/or deeper (more layers). However, don't go too far in this now, since this exploration will be the topic of the first graded assignment (with a more challenging data set).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "BJxvCocpJflO"
   },
   "outputs": [],
   "source": [
    "def build_model_hidden(inp_shape, neurons = 10, lr = 0.1,L2=0.0):\n",
    "  # a model is always built up as a list of layers\n",
    "  # In the first layer, you need to define the shape of the input\n",
    "  # all other layers automatically use the shape of the previous layer's output\n",
    "  # a linear model only has a single layer\n",
    "\n",
    "  model = keras.Sequential([\n",
    "                        keras.layers.Dense(neurons, input_shape=(inp_shape,), activation='relu', kernel_regularizer=regularizers.l2(L2)),\n",
    "                        keras.layers.Dense(neurons, activation='relu', kernel_regularizer=regularizers.l2(L2)),\n",
    "                        keras.layers.Dense(1, kernel_regularizer=regularizers.l2(L2))\n",
    "  ])\n",
    "\n",
    "  optimizer = tf.keras.optimizers.SGD(learning_rate=lr)\n",
    "\n",
    "  # When compiling the model, you have to tell it which training algorithm we will be using\n",
    "  # Details about these are not relevant at this point\n",
    "  # You also need to tell it which loss function it will be optimising \n",
    "  # and which alternative loss functions to keep track of\n",
    "  model.compile(loss='mse',\n",
    "                optimizer=optimizer,\n",
    "                metrics=['mae'])\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EUTG0AzvJ7Y0"
   },
   "outputs": [],
   "source": [
    "# call model for right number of features\n",
    "# in addition, we need to decrease the learning rate in this case\n",
    "# check for yourself what happens for larger learning rates\n",
    "model_hidden = build_model_hidden(X_train_s.shape[1],lr=0.2,neurons = 10)\n",
    "model_hidden.summary()\n",
    "\n",
    "EPOCHS = 5\n",
    "\n",
    "# Store training stats per EPOCH in history to allow plotting afterwards\n",
    "history_hidden = model_hidden.fit(X_train_s, r_train, epochs=EPOCHS,batch_size = 10,\n",
    "                    validation_data=(X_val_s,r_val), verbose=0)\n",
    "plot_history(history_hidden)\n",
    "\n",
    "[train_loss_hidden, train_mae_hidden] = model_hidden.evaluate(X_train_s, r_train, verbose=0)\n",
    "print(\"Training set Mean Squared Error:{:7.2f}\".format(train_loss_hidden))\n",
    "print(\"Training set Mean Absolute Error:{:7.2f}\\n\".format(train_mae_hidden))\n",
    "\n",
    "[val_loss_hidden, val_mae_hidden] = model_hidden.evaluate(X_val_s, r_val, verbose=0)\n",
    "print(\"Validation set Mean Squared Error:{:7.2f}\".format(val_loss_hidden))\n",
    "print(\"Validation set Mean Absolute Error:{:7.2f}\\n\".format(val_mae_hidden))\n",
    "\n",
    "[test_loss_hidden, test_mae_hidden] = model_hidden.evaluate(X_test_s, r_test, verbose=0)\n",
    "print(\"Test set Mean Squared Error:{:7.2f}\".format(test_loss_hidden))\n",
    "print(\"Test set Mean Absolute Error:{:7.2f}\\n\".format(test_mae_hidden))\n",
    "\n",
    "# We'll make a nice plot to visualise this\n",
    "# Note that this can be done a lot more elegantly, e.g. with Seaborn\n",
    "\n",
    "train_values = [train_loss_hidden, train_mae_hidden]\n",
    "val_values = [val_loss_hidden, val_mae_hidden]\n",
    "test_values = [test_loss_hidden, test_mae_hidden]\n",
    "labels = ['MSE', 'MAE']\n",
    "\n",
    "\n",
    "x=np.arange(2)\n",
    "width = 0.2  # the width of the bars\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "rects1 = ax.bar(x - width, train_values, width, label='Train')\n",
    "rects2 = ax.bar(x , val_values, width, label='Validate')\n",
    "rects3 = ax.bar(x + width, test_values, width, label='Test')\n",
    "\n",
    "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "ax.set_ylabel('Scores')\n",
    "#ax.set_title('Scores by group and gender')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(labels)\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XP-boQrOJGD9"
   },
   "source": [
    "## 5. What is different for classification\n",
    "\n",
    "For classification problems, we have discussed logistic regression as a linear model. If there are only two classes (*binary classification*), a logistic regression model only has one output, wich can take values between 0 and 1. If there are multiple classes, it has one output per class and these outputs sum up to 1. For each output, its structure is the same as that of linear regression, but with a sigmoid nonlinearity at the end. The values before the sigmoids are called the log-likelihood ratios or *logits* (we mention this term here because it sometimes occurs in the tensorflow Keras documentation and example notebooks).\n",
    "\n",
    "The most commonly used loss functions for classification are:\n",
    "- For multiple classes: categorical_crossentropy (cce) uses a one-hot array to calculate the probability, in other words, your labels need to be vectors of zeros, with a single 1 for the correct class\n",
    "- For multiple classes: sparse_categorical_crossentropy (scce) uses a category index, so your labels are numbers (starting at 0, so for 3 classes, they would be 0, 1 or 2)\n",
    "- For two classes: binary_crossentropy (labels are 0 or 1)\n",
    "\n",
    "If you use categorical_crossentropy, you will usually have to convert the original class labels to one-hot vectors before starting to train your model and take the maximum afterwards to convert the predictions back into a class number.\n",
    "\n",
    "If you use sparse_categorical_crossentropy, you will need to convert the model output to probabilities in order to analyse the mistakes your model makes and how certain it is about those mistakes.\n",
    "\n",
    "Your first and second assignments will be classification problems, and you will recieve a \"getting started\" notebook each time, so you can focus on tuning networks.\n",
    "\n",
    "## 6. Where to find more?\n",
    "\n",
    "\n",
    "There are already quite a few example notebooks on the [Tensorflow Keras website](https://www.tensorflow.org/guide/keras/overview). You should probably use these as a first source of reliable code examples. The documentation on the original Keras website is still much more extensive but not quite up to date for the most recent syntax. Although Tensorflow 2.0 is quite new, tf.keras examples will become available online quickly. **When in doubt: ask us on Slack! **\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Getting started on Colab.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
