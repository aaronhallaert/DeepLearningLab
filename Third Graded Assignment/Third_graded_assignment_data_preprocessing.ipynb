{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lcrX77WGIPYm"
   },
   "source": [
    "# Multivariate time series: data analysis and preprocessing\n",
    "\n",
    "## Dataset\n",
    "\n",
    "The data for this assignment contains time series for hourly weather parameters and pollution in the Chinese city of Bejing. The total data set contains 5 years of measurements, of which the last year is to be used as a test set.\n",
    "\n",
    "In this notebook, we show how to import and visualise the raw data and demonstrate some useful ways of preprocessing in order to make the features more useful as inputs to a neural network. \n",
    "\n",
    "**Warning:** the provided feature transformations are plausible ones, for which we provide code to help students who are less proficient in coding. It is up to you to decide which features (original or transformed) you will use in your model(s).\n",
    "\n",
    "No code for detrending is provided. As this is not the main focus of this assignment, we provide features (transformed versions of the time data) from which the network should be able to extract trends itself! Although this may not be ideal (more features = more complex network), this approach is good enough for the present assignment.\n",
    "\n",
    "## Imports and Drive mounting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dCTTphj9IPYp",
    "outputId": "a76dba68-5252-4576-a990-36923e9c4d88"
   },
   "outputs": [],
   "source": [
    "#imports and Drive mounting\n",
    "try:\n",
    "  # %tensorflow_version only exists in Colab.\n",
    "  %tensorflow_version 2.x\n",
    "except Exception:\n",
    "  pass\n",
    "\n",
    "# TensorFlow and tf.keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "print(tf.__version__)\n",
    "\n",
    "# Helper libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn as sk\n",
    "import pandas as pd\n",
    "from pandas import read_csv\n",
    "from datetime import datetime\n",
    "import math\n",
    "import os\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 2020\n",
    "np.random.seed(seed)  \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow.keras.datasets import cifar100\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, BatchNormalization, Activation\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, GlobalMaxPooling1D\n",
    "from tensorflow.keras.layers import LSTM, GRU\n",
    "from tensorflow.keras.regularizers import L1L2\n",
    "from tensorflow.keras.constraints import max_norm\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras import optimizers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wBZjBeJnIFJO"
   },
   "outputs": [],
   "source": [
    "# uncomment if you want to use Drive\n",
    "\n",
    "#from google.colab import drive\n",
    "#drive.mount('/content/gdrive')\n",
    "#\n",
    "\n",
    "##!ls '/content/gdrive/My Drive/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tNgTSTOuIPYv"
   },
   "source": [
    "## Reading the data set\n",
    "\n",
    "The data is provided as a \".csv\" file along with the getting started notebooks on Ufora. The easiest way to use it is to put it in your Drive folder and import it from there. \n",
    "\n",
    "For reading 'csv' files, using the pandas library is the easiest. This gives you back the data as a pandas dataframe. The code below imports the data and visualises the raw time series.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 201
    },
    "id": "cL_WQJJxIPYx",
    "outputId": "b6de219a-719c-4986-f1eb-76f917cbf99f"
   },
   "outputs": [],
   "source": [
    "# load dataset\n",
    "\n",
    "# The PATH setting below assumes you just uploaded the data file to your Colab session\n",
    "# When using Drive: replace this by the path where you put the data file\n",
    "DATAPATH = './'  \n",
    "\n",
    "DATAFILE = DATAPATH+'pollution_data.csv'\n",
    "dataset = read_csv(DATAFILE, header=0, index_col=0)\n",
    "dataset.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "poCpboboK7J-",
    "outputId": "e8bb7e5e-44b1-4a85-941c-393c75392fb9"
   },
   "outputs": [],
   "source": [
    "values = dataset.values\n",
    "\n",
    "# plotting the first k samples of the raw time series\n",
    "# setting below for all samples\n",
    "samples = values.shape[0]   \n",
    "# setting below for smaller number of samples to \"zoom in\", e.g., 1000\n",
    "#samples = 1000\n",
    "\n",
    "\n",
    "print(\"Dataset has \",values.shape[0],\" samples and \",values.shape[1], \" features\")\n",
    "# specify columns to plot\n",
    "groups = [0,1,2,3,4,5,6,7,8,9,10,11]\n",
    "i = 1\n",
    "# plot each column\n",
    "plt.figure(figsize=(16,26))\n",
    "for group in groups:\n",
    "    plt.subplot(len(groups), 1, i)\n",
    "    plt.plot(values[:samples, group])\n",
    "    plt.title(dataset.columns[group], y=0.5, loc='right')\n",
    "    i += 1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qqBYOg1oQqt7"
   },
   "source": [
    "## Discrete and categorical features\n",
    "\n",
    "Most of the features are numeric. However, some of those numbers are discrete:\n",
    "- year: 2010, 2011, 2012, 2013, 2014\n",
    "- month: 1, ..., 12\n",
    "- day: 1, ..., 31 (note that not all months have the same number of days)\n",
    "- hour: 0, ..., 23\n",
    "\n",
    "Wind directions are categorical values, coded as ['NE' 'NW' 'SE' 'cv'].\n",
    "\n",
    "In order to decide what to do with those, we should think about the problem again (domain knowledge). \n",
    "\n",
    "**Year:** We need to use the year in order to extract the test set further on. Aside from that, there are no indications that the pollution is systematically rising, so it will probably not be useful for detrending or as an input feature to the model.\n",
    "\n",
    "**Month:** There is a vague seasonal trend in the pollution, so knowing the month definitely could be useful. However, we can see clear seasonal trends in some of the wheather parameters (dew, temperature and pressure), so the month information may already be sufficiently present in these features. This means that in a first attempt, you could probably ignore the month feature.  \n",
    "\n",
    "If you do use the month, for example if you feel the deviation of a feature from the monthly trend is relevant, its current 'encoding' does not match very well with the observed seasonal trends, which look like shifted sine signals. Below, we show how you can re-encode the months into sine and cosine signals, with which you can approximate any shifted sinusoidal signal with the same period. You could try to use these to detrend the 'periodic' features (trend is a linear combination of sine, cosine and a constant value). Either you can also detrend the labels, or you can input sine and cosine as separate features. \n",
    "\n",
    "**Day:** The day of the month does not seem very relevant to predict pollution, but the day of the week could be. Below, we provide code examples with which you can extract the day of the week and convert it to one-hot code in two ways: one way encodes each day separately, the other only discriminates between weekend and week days. In Belgium, the second option would be very relevant, but since we don't know commuter patterns in Bejing, it may be better to keep all days. You could also plot statistical data for each weekday to check whether there is any significant difference between different days of the week.\n",
    "\n",
    "**Hour:** Time of day is probably relevant. You could use it as it is or you could consider a similar approach to what we did with the month: transforming into periodic sine and cosine signals and use these as features instead. Again, investigating the statistics for different times of day can help you decide.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 482
    },
    "id": "BN1zU_nrZCwf",
    "outputId": "f6ab746b-da86-4093-fd98-37441a6f6df2"
   },
   "outputs": [],
   "source": [
    "# Translating periodic discrete features into sines and cosines\n",
    "\n",
    "monthp = values[:,1:2]*2.0*np.pi/12.0\n",
    "month_sin = np.sin(np.array(monthp,dtype=np.float64))\n",
    "month_cos = np.cos(np.array(monthp,dtype=np.float64))\n",
    "\n",
    "#check - note that you would get a much smoother curve if you would use the day of the year\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.subplot(4,1,1)\n",
    "plt.plot(values[:,1:2])\n",
    "plt.subplot(4,1,2)\n",
    "plt.plot(month_sin)\n",
    "plt.plot(month_cos)\n",
    "plt.subplot(4,1,3)\n",
    "plt.plot(values[:,6]) # plot temp for comparison\n",
    "\n",
    "# We see that the transformed month curves are not very smooth. In fact, it is probably better to ignore the months \n",
    "# altogether and just use the day-of-the-year index to generate the periodic features\n",
    "seasons = np.reshape(np.arange(values.shape[0])*2.0*np.pi/(365.0*24),(values.shape[0],1))\n",
    "seasons_sin = np.sin(np.array(seasons,dtype=np.float64))\n",
    "seasons_cos = np.cos(np.array(seasons,dtype=np.float64))\n",
    "\n",
    "#check - note that you would get a much smoother curve if you would use the day of the year\n",
    "plt.subplot(4,1,4)\n",
    "plt.plot(seasons_sin)\n",
    "plt.plot(seasons_cos)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 372
    },
    "id": "b3tnvaWWkOub",
    "outputId": "2daf6a56-c03d-4eed-c5a4-894bf5c47a6a"
   },
   "outputs": [],
   "source": [
    "# Day of the week: take into account that 1/1/2010 was a Friday (you can look this up online)\n",
    "days = np.zeros((values.shape[0],1))\n",
    "\n",
    "# choose Monday to be day 0, then Friday is day 4\n",
    "day = 4\n",
    "for idx in range(values.shape[0]):\n",
    "  days[idx,0] = day\n",
    "  day = (day + 1) % 7 # modulo 7, so days are numbered 0-6\n",
    "\n",
    "weekdays = np.ones_like(days)\n",
    "weekdays[np.where(days>4)] = 0.0\n",
    "\n",
    "mondays = np.zeros_like(days)\n",
    "mondays[np.where(days==0)] = 1.0\n",
    "\n",
    "## check\n",
    "plt.figure(figsize=(16,6))\n",
    "plt.subplot(3,1,1)\n",
    "plt.plot(days[:100])\n",
    "plt.subplot(3,1,2)\n",
    "plt.plot(weekdays[:100])\n",
    "plt.subplot(3,1,3)\n",
    "plt.plot(mondays[:100])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mrem5jyzlbhY"
   },
   "source": [
    "We must now also convert the wind direction features to sensible numbers: possible options are integer, one-hot, 2D, polar, ...\n",
    "\n",
    "From the original paper in which this dataset was introduced, we see that the three unique categories ['NE' 'NW' 'SE' 'cv'] are actually aggregates. The first three are bucketed according to the positions of large industrial sites around Bejing, whereas the fourth corresponds to \"calm and variable\": \n",
    "\n",
    "**Source:** https://royalsocietypublishing.org/doi/10.1098/rspa.2015.0257 :\n",
    "\n",
    "**Quote:** \"* ... northwest (NW), which includes W, WNW, NW, NNW and N; northeast (NE), for NNE, NE and ENE; southeast (SE), covering E, ESE, SE, SSE and S; southwest (SW), having SSW, SW and WSW; and calm and variable (CV).*\"\n",
    "\n",
    "From this perspective, one-hot seems to be the most sensible option for encoding these. This is implemented below, as well as a version in which the one-hot encoded vectors are multiplied by wind speed. \n",
    "\n",
    "Another option would be to use one-hot encoding for the three first options ('NE', 'NW', 'SE'), and spread out the third option across all three categories, so, for example, 'NE' would be represented by [1,0,0], whereas 'cv' would be [1/3,1/3,1/3] (or these values multiplied by wind speed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 932
    },
    "id": "EYyphYWIlbuv",
    "outputId": "30644c04-57e3-41c9-b69d-8726b0fb0e6e"
   },
   "outputs": [],
   "source": [
    "# Wind direction block\n",
    "\n",
    "# creating two versions of converted feature sets for wind: \n",
    "# -  one-hot and wind speed separately (5 feature set) \n",
    "# -  4 feature set in which ones of one-hot have been replaced with wind speed\n",
    "\n",
    "\n",
    "# show unique wind direction values\n",
    "winddirs = np.unique(values[:,8])\n",
    "print(winddirs)\n",
    "\n",
    "dirindices = values[:,8]\n",
    "\n",
    "encoded_winddir = np.zeros((len(dirindices),4))\n",
    "encoded_dirstrength = np.zeros((len(dirindices),4))\n",
    "\n",
    "for idx in range(4):\n",
    "  indices = np.where(dirindices == winddirs[idx])\n",
    "  encoded_winddir[indices,idx] = 1.0\n",
    "  encoded_dirstrength[indices,idx] = values[indices,9]\n",
    "\n",
    "# check\n",
    "# Each block of two plots corresponds to a single wind direction\n",
    "# # the top of both is the one-hot version, the botton is one-hot*strength\n",
    "\n",
    "plt.figure(figsize=(16,16))\n",
    "for idx in range(4):\n",
    "    plt.subplot(8, 1, 2*idx+1)\n",
    "    plt.plot(encoded_winddir[:1000, idx])\n",
    "    plt.subplot(8, 1, 2*idx+2)\n",
    "    plt.plot(encoded_dirstrength[:1000, idx])\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n3KTEaX3m0dU"
   },
   "source": [
    "With the code above, we provide a range of possibly useful features. \n",
    "\n",
    "You will probably find that not all of these are in fact beneficial for your model. Since you do not have a lot of data and reducing the number of features is a major weapon against overfitting, you should definitely explore this. Obviously, if you get other (better) ideas, you can also generate new features yourself (and explain this in your report). \n",
    "\n",
    "After creating all features, you can join them together in a single feature matrix to use for training models. Ideally, you make a very rich feature set only once, and store it as a pandas dataframe. You can then use that as a starting point for your training notebooks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iNZJC_i6nEuS"
   },
   "outputs": [],
   "source": [
    "# helper function\n",
    "\n",
    "def combine_features(feature_list):\n",
    "  features = np.concatenate(feature_list,axis=1)\n",
    "  return features\n",
    "\n",
    "# the examples below show how you can select and combine the features you need\n",
    "# NOTE that the selection made here is ONLY an example!!\n",
    "\n",
    "\n",
    "# create 2 different wind feature sets\n",
    "fwind4 = combine_features([encoded_dirstrength])\n",
    "fwind5 = combine_features([values[:,9:10], encoded_winddir])\n",
    "\n",
    "# create wheather feature set\n",
    "fweather = values[:,5:8]\n",
    "\n",
    "# extract pollution features\n",
    "fpoll = values[:,4:5]\n",
    "\n",
    "# extract precipitation features (rain/snow)\n",
    "fwet = values[:,10:]\n",
    "\n",
    "\n",
    "# Make a feature set of weather, periodic, weekdays, wind4\n",
    "data=combine_features([fweather,seasons_cos,seasons_sin,weekdays,fwind4])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "id": "5wnuL_n55C3j",
    "outputId": "962acdc0-c296-4800-edfc-ec59821a10c3"
   },
   "outputs": [],
   "source": [
    "# you can also add the new features to the pandas dataframe and save that for later reuse\n",
    "# just a few features added here for demonstration purposes\n",
    "dataset['cos']=seasons_cos\n",
    "dataset['sin']=seasons_sin\n",
    "dataset['weekdays']=weekdays\n",
    "dataset['NE_strength']=encoded_dirstrength[:,0]\n",
    "dataset['NW_strength']=encoded_dirstrength[:,1]\n",
    "dataset['NS_strength']=encoded_dirstrength[:,2]\n",
    "dataset['cv_strength']=encoded_dirstrength[:,3]\n",
    "\n",
    "dataset.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dlWafstP5F-p",
    "outputId": "55a070eb-e275-4c6e-b6d5-3c17171a789a"
   },
   "outputs": [],
   "source": [
    "NEWDATAFILE = DATAPATH+'preprocessed_pollution_data.csv'\n",
    "dataset.to_csv(path_or_buf=NEWDATAFILE)\n",
    "\n",
    "#check\n",
    "check = read_csv(NEWDATAFILE, header=0, index_col=0)\n",
    "check.head() # should be exactly the same as dataset.head() above\n",
    "\n",
    "newvalues=check.values\n",
    "\n",
    "print(newvalues.shape)\n",
    "\n",
    "labels = check.columns\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e1p3x__fMPoh"
   },
   "source": [
    "## Splitting into train, validate and test sets\n",
    "\n",
    "In order to avoid any problems with data leakage between train and test sets, it is safest to make the split from the very beginning. As stated: the last year is the test set, the year before that is the validation set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 320
    },
    "id": "yXisA0kePl8R",
    "outputId": "e8caf4b1-22b4-44af-b621-4e5523a17311"
   },
   "outputs": [],
   "source": [
    "# The last year is meant for testing, the year of the data is in the first column\n",
    "testyear = max(newvalues[:,0])\n",
    "valyear = testyear - 1\n",
    "print(\"Val set year is \", valyear)\n",
    "print(\"Test set year is \", testyear)\n",
    "\n",
    "# Find indices of all rows in test year\n",
    "teststart = np.min(np.where(newvalues[:,0]==testyear)[0])\n",
    "valstart = np.min(np.where(newvalues[:,0]==valyear)[0])\n",
    "num_train_samples=valstart\n",
    "num_val_samples = teststart - valstart\n",
    "num_test_samples=newvalues.shape[0]-teststart\n",
    "\n",
    "print(\"There are \",num_train_samples,\" samples in the training years,\")\n",
    "print(num_val_samples,\" samples in the validation year,\")\n",
    "print(\"and \",num_test_samples,\" samples in the test year\")\n",
    "\n",
    "# extract train and test data\n",
    "train_data = pd.DataFrame(data=newvalues[:valstart,:],columns=labels)\n",
    "TRAINDATAFILE = DATAPATH+'preprocessed_train_data.csv'\n",
    "train_data.to_csv(path_or_buf=TRAINDATAFILE)\n",
    "\n",
    "val_data = pd.DataFrame(data=newvalues[valstart:teststart,:],columns=labels)\n",
    "VALDATAFILE = DATAPATH+'preprocessed_val_data.csv'\n",
    "val_data.to_csv(path_or_buf=VALDATAFILE)\n",
    "\n",
    "test_data = pd.DataFrame(data=newvalues[teststart:,:],columns=labels)\n",
    "TESTDATAFILE = DATAPATH+'preprocessed_test_data.csv'\n",
    "test_data.to_csv(path_or_buf=TESTDATAFILE)\n",
    "\n",
    "#check\n",
    "test_data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cYkNPLtmEUQT"
   },
   "source": [
    "This concludes our example code for data manipulation. Since data manipulation is not the core topic of this course, you are free to use this code as it is. However, whether you change it or not, hand in this notebook as well as the generated data files. If you do make changes, mention them in your report.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Third_graded_assignment_data_preprocessing.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
