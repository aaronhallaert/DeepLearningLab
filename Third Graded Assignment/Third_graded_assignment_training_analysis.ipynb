{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lcrX77WGIPYm"
   },
   "source": [
    "# Multivariate time series: training and analysis\n",
    "\n",
    "This notebook provides example code you can use for training and analysing your models. Note that **it is not advised** to run and train all models for this assignment in a single notebook. Ideally, in order not to loose your way in your own notebooks, you make **at least** a separate notebook for each type of model in this assignment, selecting only the code you need for that model.\n",
    "\n",
    "You can get additional inspiration from the \"Time series forecasting\" demo notebook from the Tensorflow docs:\n",
    "\n",
    "https://www.tensorflow.org/tutorials/structured_data/time_series\n",
    "\n",
    "This notebook heavily uses tensorflow utilities we haven't seen in class yet and is sometimes less easy to follow. We use some of the helper functions from that notebook in the code below.\n",
    "\n",
    "## Imports and Drive mounting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dCTTphj9IPYp",
    "outputId": "b1c59278-6d5d-44f6-888e-24ffa8b1e9e3"
   },
   "outputs": [],
   "source": [
    "#imports and Drive mounting\n",
    "try:\n",
    "  # %tensorflow_version only exists in Colab.\n",
    "  %tensorflow_version 2.x\n",
    "except Exception:\n",
    "  pass\n",
    "\n",
    "# TensorFlow and tf.keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "print(tf.__version__)\n",
    "\n",
    "# Helper libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import sklearn as sk\n",
    "import pandas as pd\n",
    "from pandas import read_csv\n",
    "from datetime import datetime\n",
    "import math\n",
    "import os\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 2020\n",
    "np.random.seed(seed)  \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow.keras.datasets import cifar100\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, BatchNormalization, Activation\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, GlobalMaxPooling1D\n",
    "from tensorflow.keras.layers import LSTM, GRU\n",
    "from tensorflow.keras.regularizers import L1L2\n",
    "from tensorflow.keras.constraints import max_norm\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.losses import MAE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cYkNPLtmEUQT"
   },
   "source": [
    "## Assignment overview\n",
    "\n",
    "The text below is de same as the text at the end of the preprocessing notebook. \n",
    "\n",
    "**However, one important piece of information was missing: we will use MAE (mean absolute error) to evaluate our models.**\n",
    "\n",
    "In your last graded assignment, you will build a model to predict pollution in a big city on past pollution info as well as past wheather info. You receive a multi-variate time series data set which contains hourly data. \n",
    "**Your task is to predict pollution 3 hours ahead** in order to be able to build a citizen warning system. The error measure you need to minimise is Mean Absolute Error. As described in the preprocessing notebook, you will **use the pollution samples from the last year for testing** and **from the one-but-last year for validation**. \n",
    "\n",
    "We already provide the simplest time series prediction baseline: using the current pollution value as a prediction.\n",
    "\n",
    "Your assignment consists of three steps (please also check the instructions in the slides):\n",
    "\n",
    "1) First make the following baseline model: a network that only predicts pollution based on the **current values of all input time series (pollution and wheather)** to predict for 3 hours ahead, i.e. a purely feature based dense model that does not take history into account. You can also use this  model to do an initial investigation of the importance of each feature! \n",
    "This baseline should at least outperform the simplest baseline.\n",
    "\n",
    "2) Then you make a second baseline that uses only **current pollution and pollution history**, so a single timeseries: a window-based dense network (the window size is an important hyperparameter to explore here).\n",
    "This baseline should at least outperform the simplest baseline.\n",
    "\n",
    "3) Now use all available information to build good time series models (using current values and histories of multiple features). Explore both, window-based and recurrent model types (cf. slides for additional instructions).  \n",
    "\n",
    "\n",
    "## Analysis\n",
    "\n",
    "Since this is a regression task, there are no misclassifications: the analysis should focus on visualising the predictions on train and validate or test sets. Code for this is suggested in the training notebook. Please make sure to not only make the necessary plots, but also to discuss them. Also provide an overview table of train, validate and test errors for teh models of each stage and a comparative discussion of model properties and performance.\n",
    "\n",
    "\n",
    "\n",
    "## Timing\n",
    "\n",
    "**This assignment is more extensive than the previous ones.** try to maximally use the experience you already gained to efficiently tune the networks. However, teh aim of this assignment is to explore and compare several approaches to time series prediction, not to try and get the very best scoring models. Once you find a network that is properly tuned, do not waste time on trying to get that extra small improvement. Try to focus on exploring and understanding which effect (if any) the different options have and link this back to theory about each model type and/or the properties of this data set. \n",
    "\n",
    "If you have time left after finishing all the tasks, you can still revisit your best model(s) to try and improve! However, be aware of the fact that this task will never be perfectly solvable, because you simply do not have enough information to perfectly predict pollution.\n",
    "\n",
    "The deadline for this assignment is **Friday, March 19, 23:30**. \n",
    "This means you have 2 weeks: for a good planning, aim at finishing at least parts 1 and 2 in the first week! That way, you can use the hands-on support session to ask us whether you are on the right track.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wBZjBeJnIFJO"
   },
   "outputs": [],
   "source": [
    "# uncomment if you want to use Drive\n",
    "\n",
    "#from google.colab import drive\n",
    "#drive.mount('/content/gdrive')\n",
    "#\n",
    "\n",
    "##!ls '/content/gdrive/My Drive/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tNgTSTOuIPYv"
   },
   "source": [
    "## Reading the data set\n",
    "\n",
    "The code below reads the train and test datasets that were generated by the preprocessing notebooks. Obviously: feel free to create your own feature sets, and to change the filenames and paths as you need.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "cL_WQJJxIPYx",
    "outputId": "9e87c175-6bfe-4a91-c6e8-bcf5db3578b7"
   },
   "outputs": [],
   "source": [
    "# load dataset\n",
    "\n",
    "# this code only loads train and test datasets\n",
    "\n",
    "# remember you also need to split off a validation dataset\n",
    "# either by reusing and adapting the splitting code from the preprocessing notebook \n",
    "# or by writing your own code\n",
    "# like in the previous assignments, this means you should have 4 sets:\n",
    "\n",
    "# train_all (train+validate), train, validate and test\n",
    "\n",
    "# The PATH setting below assumes you just uploaded the data file to your Colab session\n",
    "# When using Drive: replace this by the path where you put the data file\n",
    "DATAPATH = './'  \n",
    "TRAINDATAFILE = DATAPATH+'preprocessed_train_data.csv'\n",
    "VALDATAFILE = DATAPATH+'preprocessed_val_data.csv'\n",
    "TESTDATAFILE = DATAPATH+'preprocessed_test_data.csv'\n",
    "\n",
    "train_dataset = read_csv(TRAINDATAFILE, header=0, index_col=0)\n",
    "val_dataset = read_csv(VALDATAFILE, header=0, index_col=0)\n",
    "test_dataset = read_csv(TESTDATAFILE, header=0, index_col=0)\n",
    "\n",
    "#check - note that you can make many other plots directly from pandas dataframes:\n",
    "# https://pandas.pydata.org/pandas-docs/version/0.16/visualization.html\n",
    "\n",
    "train_dataset.plot(subplots=True,figsize=(16,26))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lR0bLHZdlEw8"
   },
   "source": [
    "## Extract train, validate and test features and labels\n",
    "\n",
    "In the code below, we will split of a validation set that is the same size as the test set (1 year) at the end of the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "poCpboboK7J-",
    "outputId": "3cc809d0-ee9e-43b0-d736-14dc901904e0"
   },
   "outputs": [],
   "source": [
    "# extract data from dataframes, selecting the features you want\n",
    "# note that the labels simply contain the pollution data for now\n",
    "# depending on the window sizes used in training, the correct values will be cut out\n",
    "\n",
    "#using indexing, you can select which features you want to use (adapt to your needs)\n",
    "features = [4,5,6,7,11,12,13,14,15,16]\n",
    "\n",
    "# index of the pollution data in the complete preprocessed feature set \n",
    "pollution = 4\n",
    "\n",
    "\n",
    "# Important: Tensorflow 2.x gives an error if you omit\n",
    "# the np.asarray(...,,dtype=np.float32)\n",
    "\n",
    "# train and validate\n",
    "train_values = np.asarray(train_dataset.values[:,features],dtype=np.float32)\n",
    "train_labels = np.asarray(train_dataset.values[:,pollution],dtype=np.float32)\n",
    "val_values = np.asarray(val_dataset.values[:,features],dtype=np.float32)\n",
    "val_labels = np.asarray(val_dataset.values[:,pollution],dtype=np.float32)\n",
    "\n",
    "# data for retraining before testing\n",
    "train_all_values = np.concatenate((train_values,val_values))\n",
    "train_all_labels = np.concatenate((train_labels,val_labels))\n",
    "\n",
    "# test data\n",
    "test_values = np.asarray(test_dataset.values[:,features],dtype=np.float32)\n",
    "test_labels = np.asarray(test_dataset.values[:,pollution],dtype=np.float32)\n",
    "\n",
    "\n",
    "# check\n",
    "print(train_values.shape)\n",
    "print(train_labels.shape)\n",
    "print(val_values.shape)\n",
    "print(val_labels.shape)\n",
    "\n",
    "print(train_all_values.shape)\n",
    "print(train_all_labels.shape)\n",
    "\n",
    "print(test_values.shape)\n",
    "print(test_labels.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t8MnvUU5Q2kK"
   },
   "source": [
    "## Normalising the data\n",
    "\n",
    "As you know by now, it is important to normalise the data. The flow is the following:\n",
    "\n",
    "- initialise normaliser 1\n",
    "- fit on training date\n",
    "- use it to transform train and validation data\n",
    "\n",
    "For your final model(s) you need a second normaliser:\n",
    "- initialise normaliser 2\n",
    "- fit on training+validation data\n",
    "- use it to transform training+validation and test data\n",
    "\n",
    "This has to be done **before** reformatting the data into windows (next section).  \n",
    "\n",
    "Although we already did this before, the code below gives an example, The imports show that there are many different scalers. Especially with recurrent NNs, the choice of scaler can make a difference. Look up what each scaler does before choosing which one to try. Also, recurrent models may need different scalers than other models. Finally, you could also consider to start with a batchnorm layer to let your model fine-tune the feature scaling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kJASfvPcTVzN"
   },
   "outputs": [],
   "source": [
    "# imports to show that there are many different scalers\n",
    "# especially with recurrent NNs, the choice of scaler can make a difference\n",
    "# look up what they do before choosing which one to try\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "# Example: train standard scalers, apply to train and test data\n",
    "# adapt to do all you need to do ...\n",
    "\n",
    "SS1 = StandardScaler()\n",
    "SS1.fit(train_values)\n",
    "\n",
    "train_scaled = SS1.transform(train_values)\n",
    "val_scaled = SS1.transform(val_values)\n",
    "\n",
    "SS2 = StandardScaler()\n",
    "SS2.fit(train_all_values)\n",
    "\n",
    "train_all_scaled = SS2.transform(train_all_values)\n",
    "test_scaled = SS2.transform(test_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TPCRAAhBQIc6"
   },
   "source": [
    "## Transforming the data into the right format for model training\n",
    "\n",
    "In this assignment, you will use models with two types of inputs: \n",
    "- a dense network that only uses the current input features for prediction\n",
    "- window-based and sequential networks that use time windows for prediction\n",
    "\n",
    "In the code below, we define a helper function for each case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qUHwpucaCGcs",
    "outputId": "ea1354fe-31ed-46e8-d581-21468c35db78"
   },
   "outputs": [],
   "source": [
    "# dense network:\n",
    "# here you only need to take into account that we are predicting 3 steps ahead\n",
    "# this means that the features of the first timestep (index 0)\n",
    "# are used to predict the 4th pollution value (index 3) so the first 3 and the last 3 feature-samples labels are omitted\n",
    "\n",
    "def create_dataset_dense(train, test, ahead=3):   # can use this with different 'ahead' values, but default is set to 3\n",
    "    return train[:-ahead,:], test[ahead:]\n",
    "\n",
    "X_train_d,r_train_d = create_dataset_dense(train_scaled,train_labels)\n",
    "X_val_d,r_val_d = create_dataset_dense(val_scaled,val_labels)\n",
    "\n",
    "X_train_all_d,r_train_all_d = create_dataset_dense(train_all_scaled,train_all_labels)\n",
    "X_test_d,r_test_d = create_dataset_dense(test_scaled,test_labels)\n",
    "\n",
    "\n",
    "print(X_train_d.shape)\n",
    "print(r_train_d.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1tIqYyPdEON4",
    "outputId": "e124754d-6b23-4623-cd58-d5b85f2ff2cc"
   },
   "outputs": [],
   "source": [
    "# window-based and recurrent networks:\n",
    "# now, you use a window of k history values to predict\n",
    "# this means that the features of the first k timesteps (indices 0 to k-1)\n",
    "# are used to predict the k+3rd pollution value (index k+3-1)\n",
    "# output dimension of train data is samples x window x features\n",
    "\n",
    "def create_dataset_windowed(train, test, ahead=3, window_size=1):\n",
    "  samples = train.shape[0]-ahead-(window_size-1) \n",
    "  dataX= []\n",
    "  for i in range(samples):\n",
    "      a = train[i:(i+window_size), :]\n",
    "      dataX.append(a)\n",
    "  return np.array(dataX), test[ahead+window_size-1:]\n",
    "\n",
    "# set chosen window size\n",
    "WINDOW=5\n",
    "X_train_w,r_train_w = create_dataset_windowed(train_scaled,train_labels, window_size=WINDOW)\n",
    "X_val_w,r_val_w = create_dataset_windowed(val_scaled,val_labels, window_size=WINDOW)\n",
    "\n",
    "X_train_all_w,r_train_all_w = create_dataset_windowed(train_all_scaled,train_all_labels, window_size=WINDOW)\n",
    "X_test_w,r_test_w = create_dataset_windowed(test_scaled,test_labels, window_size=WINDOW)\n",
    "\n",
    "print(X_train_w.shape)\n",
    "print(r_train_w.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3oruQX2JKg_z"
   },
   "source": [
    "## Simplest sandbox model\n",
    "\n",
    "The simplest sandbox model you can make just uses the current pollution to predict the pollution of 6 hours ahead. In the code below, we create these predictions, and evaluate them, using a 'Plotresults' function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vcUCOh4TWsgV",
    "outputId": "153874bf-ca9c-4e5d-f0b2-2428a149394b"
   },
   "outputs": [],
   "source": [
    "# We use current pollution as prediction for the \"dense\" labels\n",
    "\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "y_train_current = train_values[:-3,0]\n",
    "y_val_current = val_values[:-3,0]\n",
    "y_test_current = test_values[:-3,0]\n",
    "\n",
    "mae_train_current = mean_absolute_error(r_train_d,y_train_current)\n",
    "mae_val_current = mean_absolute_error(r_val_d,y_val_current)\n",
    "mae_test_current = mean_absolute_error(r_test_d,y_test_current)\n",
    "\n",
    "print(\"\\\"Current pollution\\\" baseline:\\n train mae = \",mae_train_current,\n",
    "      \"\\n validation mae = \",mae_val_current,\n",
    "      \"\\n test mae = \",mae_test_current)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g-9MPMKH7_UZ"
   },
   "source": [
    "The results show a pretty close match between train, validate and test scores. This can be expected, since this model involved no training. However, the small discrepancy between the validation set and the other two sets suggests that the validation year may contain some slightly more difficult samples (so not quite i.i.d.). Keep thin in mind when interpreting the performance of your models.\n",
    "\n",
    "Finally, below, we give examples of how you can visualise the results for regression models. Feel free to come up with more interesting analyses if you can."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 930
    },
    "id": "fR45InWra02f",
    "outputId": "8ce30816-7a28-4686-a58b-f7ee9bb8ebcb"
   },
   "outputs": [],
   "source": [
    "def PlotResults(labels,predictions,binsize = 10):\n",
    "  num_samples = len(labels)\n",
    "  \n",
    "  fig = plt.figure(figsize=(16,16))\n",
    "  spec = gridspec.GridSpec(ncols=4, nrows=4, figure=fig)\n",
    "  ax1 = fig.add_subplot(spec[0, :])\n",
    "  ax2 = fig.add_subplot(spec[1, :])\n",
    "  ax3 = fig.add_subplot(spec[2:,0:2])\n",
    "  ax4 = fig.add_subplot(spec[2:,2:])\n",
    "  \n",
    "  ax1.plot(labels,'k-',label='Labels')\n",
    "  ax1.plot(predictions,'r-',label='Predictions')\n",
    "  ax1.set_ylabel('Pollution')\n",
    "  ax1.legend()\n",
    "  \n",
    "  errors=np.absolute(labels-predictions)\n",
    "  ax2.plot(errors,'k-')\n",
    "  ax2.set_ylabel(\"Absolute error\")\n",
    "  \n",
    "  ax3.scatter(labels,predictions)\n",
    "  ax3.set_xlabel('Labels')\n",
    "  ax3.set_ylabel('Predictions')\n",
    "\n",
    "  bins = np.arange(0,(np.ceil(np.max(errors)/binsize)+1)*binsize,binsize)\n",
    "  \n",
    "  ax4.hist(errors,bins=bins)\n",
    "  ax4.set_xlabel('Absolute error')\n",
    "  ax4.set_ylabel('Frequency')\n",
    "\n",
    "  plt.show()\n",
    "\n",
    "# Visualise first 1000 predictions\n",
    "PlotResults(r_train_d[:1000],y_train_current[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6a_t90TYkvJi"
   },
   "source": [
    "## Building models\n",
    "\n",
    "The code below offers a quick and dirty overview of how to construct and train models of different types.\n",
    "\n",
    "When working in your assignment, make a new notebook for each type of model (and remove code that is not relevant for that model)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6aEwBiUtrZMi",
    "outputId": "154becdc-7910-46ff-8cfe-47500b7ee247"
   },
   "outputs": [],
   "source": [
    "print(X_val_d.shape)\n",
    "print(r_val_d.shape)\n",
    "\n",
    "\n",
    "# Stage 1 model: dense model only using current features for prediction\n",
    "# uses datasets with '_d' \n",
    "\n",
    "\n",
    "\n",
    "# put model in a function wrapper\n",
    "# this model assumes dropout on all layers, including the input layer\n",
    "# without specifying any hidden layers this is just linear regression\n",
    "\n",
    "# note that this will give an error if the number of dropouts does not equal the number of hiddens+1\n",
    "# and as always: feel free to use your own functions\n",
    "\n",
    "def my_Dense(num_features,learning_rate=0.001,hidden=[],ridge_param = 0.0,dropouts=[0.0]):\n",
    "    # create linear modelmodel\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dropout(dropouts[0], input_shape=(num_features,)))\n",
    "    for idx in range(len(hidden)):\n",
    "      model.add(Dense(hidden[idx],activation = 'ReLu',kernel_regularizer=tf.keras.regularizers.l2(ridge_param)))\n",
    "      model.add(Dropout(dropouts[idx+1]))\n",
    "    model.add(Dense(1,activation='linear',kernel_regularizer=tf.keras.regularizers.l2(ridge_param)))\n",
    "    \n",
    "    optim = tf.keras.optimizers.Adam(lr=learning_rate)\n",
    "\n",
    "    # Compile model\n",
    "    # Note that Tensorflow allows you to directly optimise MAE during training!\n",
    "    \n",
    "    model.compile(loss='mae', optimizer=optim, metrics=['mae','mse']) # keep extra metrics: here mse and mae without regularisation terms\n",
    "    return model    \n",
    "\n",
    "\n",
    "linreg = my_Dense(X_train_d.shape[1],learning_rate=0.01)\n",
    "\n",
    "# and check whether everything is as you expected\n",
    "linreg.summary()\n",
    "\n",
    "batch_size=64\n",
    "epochs=30\n",
    "linreg_history = linreg.fit(X_train_d, r_train_d,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(X_val_d, r_val_d),\n",
    "          shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 277
    },
    "id": "JQe_OoLEaZA-",
    "outputId": "59080d31-ed32-489d-c2ad-53020f1f1ab4"
   },
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "  plt.figure(figsize = (6,4))\n",
    "  \n",
    "  plt.xlabel('Epoch')\n",
    "  plt.ylabel('Mae')\n",
    "  plt.plot(history.epoch, np.array(history.history['mae']),'g-',\n",
    "           label='Train MAE')\n",
    "  plt.plot(history.epoch, np.array(history.history['val_mae']),'r-',\n",
    "           label = 'Validation MAE')\n",
    "  plt.legend()\n",
    "  plt.show()\n",
    "\n",
    "plot_history(linreg_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "IwY7Fkn5Z6Iy",
    "outputId": "aa3d0dc0-a9f5-44d1-a028-b5856f66b7fb"
   },
   "outputs": [],
   "source": [
    "y_train_linreg = linreg.predict(X_train_d)\n",
    "y_val_linreg = linreg.predict(X_val_d)\n",
    "\n",
    "mae_train_linreg = mean_absolute_error(r_train_d,y_train_linreg)\n",
    "mae_val_linreg = mean_absolute_error(r_val_d,y_val_linreg)\n",
    "\n",
    "print(\"\\\"Current pollution\\\" baseline:\\n train mae = \",mae_train_linreg,\n",
    "      \"\\n validation mae = \",mae_val_linreg)\n",
    "\n",
    "print(y_train_linreg.shape)\n",
    "print(y_val_linreg.shape)\n",
    "\n",
    "# Visualise first 1000 predictions for validation\n",
    "PlotResults(r_val_d[:1000],y_val_linreg[:1000,0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kLdveBe7eFpP"
   },
   "source": [
    "The model above is pure linear regression. We see it is still 'lagging behind' almost as much as the previous baseline. It did not use all the features, and certainly not all the options that were provided in the preprocessing notebook. Using a slightly more complex model, the error can probably be reduced a bit. However, since the instantaneous model is no more than a baseline, don't spend too much time on it!\n",
    "\n",
    "The next models you need to tune are window-based models: the pollution-only baseline (dense, but you are free to try conv as well) and the 1Dconv model with multiple features. The code below demonstrates how to set up conv models. For dense models, you use the same windowed dataset creation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "94-oah8Ee168",
    "outputId": "98e5cb84-49eb-4db9-a853-3adaaf67122d"
   },
   "outputs": [],
   "source": [
    "CNN_WINDOW_SIZE = 24\n",
    "\n",
    "# current code for pollution-only, add other features for multi-feature model, \n",
    "# remove indices as in commented code if you want to use all features\n",
    "\n",
    "# note that in the current feature set, pollution is the first feature (index=0)\n",
    "\n",
    "train_features = [0,] \n",
    "\n",
    "X_train_w,r_train_w = create_dataset_windowed(train_scaled[:,train_features],train_labels, window_size=CNN_WINDOW_SIZE)\n",
    "X_val_w,r_val_w = create_dataset_windowed(val_scaled[:,train_features],val_labels, window_size=CNN_WINDOW_SIZE)\n",
    "\n",
    "X_train_all_w,r_train_all_w = create_dataset_windowed(train_all_scaled[:,train_features],train_all_labels, window_size=CNN_WINDOW_SIZE)\n",
    "X_test_w,r_test_w = create_dataset_windowed(test_scaled[:,train_features],test_labels, window_size=CNN_WINDOW_SIZE)\n",
    "\n",
    "\n",
    "CNNmodel = Sequential()\n",
    "CNNmodel.add(Conv1D(input_shape = (CNN_WINDOW_SIZE, X_train_w.shape[-1]), \n",
    "                        filters=32,\n",
    "                        kernel_size=2,\n",
    "                        padding='same',\n",
    "                        activation='relu'))\n",
    "CNNmodel.add(MaxPooling1D(pool_size=2))\n",
    "CNNmodel.add(Conv1D(filters=16,\n",
    "                        kernel_size=2,\n",
    "                        padding='same',\n",
    "                        activation='relu'))\n",
    "CNNmodel.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "CNNmodel.add(Flatten())\n",
    "CNNmodel.add(Dense(1))\n",
    "CNNmodel.add(Activation('linear'))\n",
    "\n",
    "CNNmodel.summary()\n",
    "\n",
    "CNNmodel.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),loss='mae', metrics=['mae','mse'])\n",
    "\n",
    "batch_size=64\n",
    "epochs=10\n",
    "CNN_history = CNNmodel.fit(X_train_w, r_train_w, \n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(X_val_w, r_val_w),\n",
    "          shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 277
    },
    "id": "NOkB_tetjOYX",
    "outputId": "984b8e91-ade5-4a5f-9ff3-96cad56d1b07"
   },
   "outputs": [],
   "source": [
    "plot_history(CNN_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "2q7MylLdiaUt",
    "outputId": "b9805eb5-899a-46cf-c71c-cc6fdbb6b55d"
   },
   "outputs": [],
   "source": [
    "y_train_CNNmodel = CNNmodel.predict(X_train_w)\n",
    "y_val_CNNmodel = CNNmodel.predict(X_val_w)\n",
    "\n",
    "mae_train_CNNmodel = mean_absolute_error(r_train_w,y_train_CNNmodel)\n",
    "mae_val_CNNmodel = mean_absolute_error(r_val_w,y_val_CNNmodel)\n",
    "\n",
    "print(\"\\\"Current pollution\\\" baseline:\\n train mae = \",mae_train_CNNmodel,\n",
    "      \"\\n validation mae = \",mae_val_CNNmodel)\n",
    "\n",
    "print(y_train_CNNmodel.shape)\n",
    "print(y_val_CNNmodel.shape)\n",
    "\n",
    "# Visualise first 1000 predictions for validation\n",
    "PlotResults(r_val_w[:1000],y_val_CNNmodel[:1000,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iJT8-13tjdfu"
   },
   "source": [
    "When tuning your convolutional model, try to think about the receptive fields of the features you extract (cf. lecture). The current network only has two layers of 2x2 filters, so the receptive fields are very small!!\n",
    "\n",
    "The final model type is a recurrent model, which is demonstrated in the code below. Remember that LSTMs and GRUs require the input data to be scaled between 0 and 1!! Since currently, a standardscaler was used, the code below does not give useful results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Droj_gM-mGpI",
    "outputId": "d7bb2973-d094-46e7-dc6b-8cbb9a17a62a"
   },
   "outputs": [],
   "source": [
    "HIDDEN_RNN = 4\n",
    "RNN_WINDOW_SIZE = 8\n",
    "\n",
    "# current code for pollution-only, add other features for multi-feature model, \n",
    "# remove indices as in commented code if you want to use all features\n",
    "train_features = [0,] \n",
    "\n",
    "X_train_w,r_train_w = create_dataset_windowed(train_scaled[:,train_features],train_labels, window_size=RNN_WINDOW_SIZE)\n",
    "X_val_w,r_val_w = create_dataset_windowed(val_scaled[:,train_features],val_labels, window_size=RNN_WINDOW_SIZE)\n",
    "\n",
    "X_train_all_w,r_train_all_w = create_dataset_windowed(train_all_scaled[:,train_features],train_all_labels, window_size=RNN_WINDOW_SIZE)\n",
    "X_test_w,r_test_w = create_dataset_windowed(test_scaled[:,train_features],test_labels, window_size=RNN_WINDOW_SIZE)\n",
    "\n",
    "# regularisers - all set to zero for now\n",
    "wreg = L1L2(l1=0.0, l2=0.0)\n",
    "\n",
    "\n",
    "GRUmodel = Sequential()\n",
    "\n",
    "# Note that you can also make multi-layer GRUs \n",
    "# Since these send their output to another GRU, they should output whole sequences \n",
    "# instead of predictions\n",
    "# For this reason, all but the last GRU layer should have return_sequences=True\n",
    "\n",
    "GRUmodel.add(GRU(input_shape = (RNN_WINDOW_SIZE,1), \n",
    "                   units=HIDDEN_RNN,\n",
    "                   return_sequences=False, \n",
    "                   kernel_regularizer=wreg))\n",
    "GRUmodel.add(Dense(10,activation='relu')) # note that a different nonlinearity may be useful here\n",
    "GRUmodel.add(Dense(1))\n",
    "GRUmodel.summary()\n",
    "\n",
    "GRUmodel.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),loss='mae', metrics=['mae','mse'])\n",
    "\n",
    "\n",
    "batch_size=64\n",
    "epochs=10\n",
    "GRU_history = GRUmodel.fit(X_train_w, r_train_w, \n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(X_val_w, r_val_w),\n",
    "          shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 277
    },
    "id": "6MLvJbzFnTuu",
    "outputId": "f54af607-c292-4b8d-9d63-0d9b76ccd38a"
   },
   "outputs": [],
   "source": [
    "plot_history(GRU_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 980
    },
    "id": "zMdWxYHInaZ-",
    "outputId": "570e8995-bd3b-4fd9-c3b9-5586aa688ba0"
   },
   "outputs": [],
   "source": [
    "y_train_GRUmodel = GRUmodel.predict(X_train_w)\n",
    "y_val_GRUmodel = GRUmodel.predict(X_val_w)\n",
    "\n",
    "mae_train_GRUmodel = mean_absolute_error(r_train_w,y_train_GRUmodel)\n",
    "mae_val_GRUmodel = mean_absolute_error(r_val_w,y_val_GRUmodel)\n",
    "\n",
    "print(\"\\\"Current pollution\\\" baseline:\\n train mae = \",mae_train_GRUmodel,\n",
    "      \"\\n validation mae = \",mae_val_GRUmodel)\n",
    "\n",
    "# Visualise first 1000 predictions for validation\n",
    "PlotResults(r_val_w[:1000],y_val_GRUmodel[:1000,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Zy73dOzr0RR"
   },
   "source": [
    "Now it's up to you to try and improve these models and understand, for this particular problem, what would work and what wouldn't!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Third_graded_assignment_training_analysis.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
