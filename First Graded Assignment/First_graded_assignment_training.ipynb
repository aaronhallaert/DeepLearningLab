{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kyD1qjBA3RfY"
   },
   "source": [
    "# First graded assignment\n",
    "\n",
    "The aim of this first graded assignment is to practice building and tuning neural networks in Tensorflow Keras, analysing their performance and making the right conclusions based on these analyses. The dataset for this assignment is the MNIST handwritten digit classification data set. A description of this data set is given here https://en.wikipedia.org/wiki/MNIST_database. \n",
    "\n",
    "MNIST consists of grayscale images of 28x28 pixels, representing (nicely centered and similarly scaled) handwritten digits (0 - 9). This data set is often used as a benchmark for machine learning techniques. It is standardised, i.e. the train and test sets have been fixed in order to make test results comparable. This data set is also balanced: it contains an equal number of examples of each class. The Wikipedia page also shows the achieved results for a number of techniques. For example, it mentions an error rate of 1.6 (i.e., an accuracy of  0.984) for a neural network with 2 layers.\n",
    "\n",
    "In this notebook, we provide example code to read the data and a first network to show how to address a multi-class classification problem in Tensorflow Keras. We also show how to save your trained model to Google Drive.\n",
    "\n",
    "In a second notebook, we show how to load your trained model and analyse its performance on the test. You can plota confusion matrix and visualise a given sample as an image, e.g. to analyse which samples have been misclassified. Especially when you are working with images, it is important to look at (some examples of) your data, as well as at the errors your model makes.\n",
    "\n",
    "\n",
    "## Instructions (see slides for more info)\n",
    "\n",
    "1. In this assignment, you will design a neural network to classify the handwritten digits from the MNIST data set using **only the techniques you have seen thus far**:\n",
    "- only use networks consisting of 1 or multiple fully connected layers (\"Dense\" layers in Tensorflow) with ReLu neurons, explore number of neurons per layer and number of layers\n",
    "- adapt (initial) learning rate, batch size and training duration (epochs) to ensure learning convergence - use early stopping (model checkpoints)\n",
    "- explore L2 and/or L1 regularisation, maxnorm and dropout as regularisation options\n",
    "\n",
    "\n",
    "2. You should try to follow a systematic approach in tuning your network(s), in accordance with the guidelines discussed in class. In order to show this, you will motivate your decisions by analysis results: plots and numbers, as discussed in class. For each step, diagnose your network (i.e., over/underfitting, learning convergence OK or not, ...) and motivate your next step.\n",
    "\n",
    "3. After tuning your network on a validation set, you will fix the chosen network structure and hyperparameters and retrain on the entire train data from the original set. You will then evaluate your model on the test set and analyse and discuss the results: which mistakes does your model still make? Are these acceptable? Does your network show \"doubt\" for hard cases or is it confident of its wrong predictions?\n",
    "\n",
    "For instructions on how to document your work and what to hand in: see lecture 3 and companion slides. \n",
    "\n",
    "**The deadline for this assignment is Friday, February 26th (before 23:30)!**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49
    },
    "colab_type": "code",
    "id": "E8ZDx4K73Rfb",
    "outputId": "1dfd8e06-f694-4580-e32a-8bd27f487416"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "  # %tensorflow_version only exists in Colab.\n",
    "  %tensorflow_version 2.x\n",
    "except Exception:\n",
    "  pass\n",
    "\n",
    "# TensorFlow and tf.keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "print(tf.__version__)\n",
    "\n",
    "# Helper libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn as sk\n",
    "import pandas as pd\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 2021\n",
    "np.random.seed(seed)  \n",
    "\n",
    "import sklearn as sk\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.constraints import max_norm\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rL5nGo4s-PyD"
   },
   "outputs": [],
   "source": [
    "#helper functions for visualisation\n",
    "\n",
    "# same function as in the getting started notebook, \n",
    "# but now plotting the loss functions used in this notebook\n",
    "# we plot the loss we want to optimise on th eleft (in this case: accuracy)\n",
    "def plot_history(history):\n",
    "  plt.figure(figsize = (12,4))\n",
    "  plt.subplot(1,2,1)\n",
    "\n",
    "  plt.xlabel('Epoch')\n",
    "  plt.ylabel('Accuracy')\n",
    "  plt.plot(history.epoch, np.array(history.history['accuracy']),'g-',\n",
    "           label='Train accuracy')\n",
    "  plt.plot(history.epoch, np.array(history.history['val_accuracy']),'r-',\n",
    "           label = 'Validation accuracy')\n",
    "  plt.legend()\n",
    "\n",
    "  plt.subplot(1,2,2)\n",
    "  plt.xlabel('Epoch')\n",
    "  plt.ylabel('Loss minimised by model')\n",
    "  plt.plot(history.epoch, np.array(history.history['loss']),'g-',\n",
    "           label='Train loss')\n",
    "  plt.plot(history.epoch, np.array(history.history['val_loss']),'r-',\n",
    "           label = 'Validation loss')\n",
    "  plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M8zD7f-brnYi"
   },
   "source": [
    "## Mounting your Google Drive\n",
    "\n",
    "Execute the code below to make your Google Drive accessible for your notebook from Colab. You will be asked to follow a link where you can login to your Google account.\n",
    "If this is successfull, you will see an authorization code which you will be asked to fill in below (copy and paste - it's quite long)\n",
    "\n",
    "The last line of this code block should show you what is on your drive.\n",
    "\n",
    "Do not use this code when working on your own laptop!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 118
    },
    "colab_type": "code",
    "id": "OiWCoaZIrzpj",
    "outputId": "393c25d3-d969-45af-bb5b-020694199a24"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')\n",
    "\n",
    "import os\n",
    "\n",
    "#!ls '/content/gdrive/My Drive/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5BX66EkHVF5n"
   },
   "source": [
    "## Loading the data\n",
    "\n",
    "In the code below, the MNIST data is loaded and the features are rescaled. In this case we don't need a trainable scaler since the features are pixel values, i.e., integers between 0 and 155. We rescale them to be in [0,1] since this often works better with many neural network architectures.\n",
    "\n",
    "There already is a predefined test set, but we split off a validation set (from the training data) of 10000 samples.\n",
    "\n",
    "Finally, often a subset of the training data is used for an initial architecture exploration.  The code below shows how to do this, in this case by setting N_train to 10000 (roughly 1000 samples per class). **Do not forget to switch to the complete training set once you start tuning a promising architecture!!** (do this by uncommenting the line in which N_train is reduced from its original value)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 115
    },
    "colab_type": "code",
    "id": "UxP4HNF-3Rfh",
    "outputId": "9f94d93a-e5cb-43c4-f252-a419545f0b3a"
   },
   "outputs": [],
   "source": [
    "# load train and test data\n",
    "(x_train_all, r_train_all_class), (x_test, r_test_class) = mnist.load_data()\n",
    "x_train_all = x_train_all.reshape(x_train_all.shape[0], -1)\n",
    "x_test = x_test.reshape(x_test.shape[0], -1)\n",
    "\n",
    "# some preprocessing ... convert integers to floating point and rescale them to [0,1] range\n",
    "x_train_all = x_train_all.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train_all /= 255\n",
    "x_test /= 255\n",
    "\n",
    "print(x_train_all.shape[0], ' original train samples')\n",
    "print(x_test.shape[0], ' original test samples')\n",
    "\n",
    "# This data set contains a train set and test set\n",
    "# we still need to split off a validation set\n",
    "\n",
    "# Number of test samples\n",
    "N_test = x_test.shape[0]\n",
    "\n",
    "# split off 10000 samples for validation\n",
    "N_val = 10000\n",
    "N_train = x_train_all.shape[0] - N_val\n",
    "\n",
    "# now extract the samples into train, validate and test sets\n",
    "# set random state to make sure you get the same split each time\n",
    "x_train, x_val, r_train_class, r_val_class = train_test_split(x_train_all, r_train_all_class, test_size = N_val, random_state=0)\n",
    "\n",
    "# For initial explorations, it is often useful to \n",
    "# try out some things first on a smaller data set\n",
    "# in comment below is the code for using all training data\n",
    "# in this case, we select 10000 samples for training\n",
    "# don't forget to use all training examples for your final model\n",
    "# by replacing the line below by the commented one beneath it\n",
    "\n",
    "N_train = 10000\n",
    "\n",
    "x_train = x_train[:N_train,:]\n",
    "r_train_class = r_train_class[:N_train]\n",
    "\n",
    "print(\"Using \",x_train.shape[0],\" train samples, \", x_val.shape[0], \" validation samples and \",x_test.shape[0],\" test samples\")\n",
    "print(\"Each sample has \",x_train.shape[1],\" features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lfQXV8EAWm8K"
   },
   "source": [
    "\n",
    "In the code below, we visualise a few training samples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 605
    },
    "colab_type": "code",
    "id": "xATelOMr3Rfl",
    "outputId": "ace23059-8bd6-4c1f-e0c4-ac3909bde93e"
   },
   "outputs": [],
   "source": [
    "# The features in this data set are the pixels of a 28x28 pixel image\n",
    "# You can visualise an individual image as follows\n",
    "# (here for the first 9 images in the training set)\n",
    "\n",
    "f = plt.figure(figsize=(10,10));\n",
    "for idx in range(9):\n",
    "    plt.subplot(3,3,idx+1)\n",
    "    plt.subplots_adjust(hspace=0.5)\n",
    "    plt.title(\"Label is \" + str(r_train_class[idx]))\n",
    "    plt.imshow(np.reshape(x_train[idx,:],(28,28)), cmap='Greys', interpolation='None')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49
    },
    "colab_type": "code",
    "id": "GYagr-4C3Rfp",
    "outputId": "14ae3b8d-a8a8-42b5-9fca-f8fc2b28ecb2"
   },
   "outputs": [],
   "source": [
    "# The labels are numbers from 0 to 9\n",
    "print(\"A few labels:\")\n",
    "print(r_train_class[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Px0ZNPE23Rft"
   },
   "source": [
    "## Multi-class classification\n",
    "\n",
    "For multi-class classification problems with k classes, we train one model output per class. Each of the k outputs represents the probability that the sample comes from that class. This means the target labels are vectors of k values, all of which are zero except for that which corresponds to the correct class.\n",
    "\n",
    "There are multiple ways to achieve this. Here, we will convert class vectors to binary class matrices instead of 1 integer label, you now have 10 binary labels (1 for each class) these labels represent the desired class probabilities: all labels are zero, except the one for the right class, which is one.\n",
    "\n",
    "This will result in a model that outputs vectors of 10 probabilities when calling its 'predict()' function. In the analysis notebook, we will obtain class labels by finding the largest probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 32
    },
    "colab_type": "code",
    "id": "x5xdDCBW3Rfu",
    "outputId": "37b868f8-4ef5-42bf-948b-029553150fe1"
   },
   "outputs": [],
   "source": [
    "r_train_all = keras.utils.to_categorical(r_train_all_class)\n",
    "r_train = keras.utils.to_categorical(r_train_class)\n",
    "r_val = keras.utils.to_categorical(r_val_class)\n",
    "r_test = keras.utils.to_categorical(r_test_class)\n",
    "\n",
    "# look at the new labels for the first sample\n",
    "print(r_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RHh6Ju04YH2v"
   },
   "source": [
    "For classification, the output layer ends in a sigmoid activation. However, for multiple classes, it needs to be ensured that the outputs sum up to 1 (since they represent class probabilities). This is achieved with the 'softmax' activation.\n",
    "\n",
    "Since we are addressing a multi-class classification problem, with vectors of zeros and ones as labels, the loss function is 'categorical cross entropy'. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d2oefgBo3Rfz"
   },
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "\n",
    "# this first network has 2 hidden layers\n",
    "# the first layer needs to be told explicitly what the input shape is\n",
    "# the output layer has 10 neurons: one neuron per class\n",
    "\n",
    "# Note that we use the \"He\" initialisation scheme here, since this is often advised \n",
    "# for layers with ReLu neurons - although you are free to change this, it is not necessary for this assignment\n",
    "\n",
    "# Also note that \"dropout\" is implemented in separate layers in Keras\n",
    "# they are added below in comment to show you how to use them\n",
    "# note that you can also start your network with a dropout layer (randomly setting input features to 0)\n",
    "\n",
    "def initial_model():\n",
    "    # create linear model\n",
    "    model = Sequential()\n",
    "    # we start with a first fully connected layer\n",
    "    model.add(Dense(16, activation='relu', input_shape=(784,), kernel_initializer='he_uniform'))\n",
    "    ## then add some dropout, set at a very low value for now\n",
    "    #model.add(Dropout(0.001))\n",
    "    # a second dense layer with half as many neurons\n",
    "    model.add(Dense(16, activation='relu', kernel_initializer='he_uniform'))\n",
    "    ## some more dropout\n",
    "    #model.add(Dropout(0.001))\n",
    "    # and the output layer\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=Adam(learning_rate=0.001), # set to default learning rate here\n",
    "                  metrics=['accuracy']) # save accuracy in addition to cross entropy error\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YYVqgcdecK7j"
   },
   "source": [
    "## Training \n",
    "\n",
    "The code below will introduce a few new aspects (compared to the 'getting started' notebook):\n",
    "- early stopping\n",
    "- keeping track of your best model during training and saving it\n",
    "- saving a model after training\n",
    "\n",
    "For the last two, you need to specify where the model should be saved:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "DZJEehAQ3Rf6",
    "outputId": "023e23b1-7adc-40ca-a304-f4bf46f46507"
   },
   "outputs": [],
   "source": [
    "# Model checkpoints are saved versions of intermediate models during training\n",
    "\n",
    "# Set the path for saving intermediate model checkpoints\n",
    "\n",
    "# With the setting below, models will be dumped in your drive in the specified folder (make this folder first)\n",
    "# Change this path name to a unique path for each model you want to keep (otherwise they will be overwritten)\n",
    "\n",
    "checkpoint_dir = \"/content/gdrive/My Drive/Colab Notebooks/DL2021/GA1/\"\n",
    "print(\"Models will be saved in \",checkpoint_dir)\n",
    "# Have a look what is already in that path\n",
    "!ls \"{checkpoint_dir}\"\n",
    "\n",
    "model_savename = checkpoint_dir+\"initialmodel_model_version_0.1.h5\"\n",
    "checkpoint_path = checkpoint_dir+\"cp-{epoch:04d}.ckpt\"\n",
    "\n",
    "\n",
    "# Create your model\n",
    "model_1 = initial_model()\n",
    "model_1.summary()\n",
    "\n",
    "\n",
    "# We now add batch size to the mix of training parameters\n",
    "# If you don't specify batch size below, all training data will be used for each learning step\n",
    "batch_size = 16\n",
    "epochs = 20\n",
    "\n",
    "# Modelcheckpoint callback:\n",
    "# - save at every epoch if 'save_best_only'=false\n",
    "# - save complete model if 'save weights only' = false\n",
    "\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(checkpoint_path, \n",
    "                                                 monitor='val_accuracy',\n",
    "                                                 save_weights_only=True,\n",
    "                                                 save_best_only=True,\n",
    "                                                 verbose=1)\n",
    "\n",
    "# Set early stopping: with parameters below, training will stop \n",
    "# when validation accuracy hasn't improved for 6 epochs\n",
    "# (or when the total number of epochs has passed)\n",
    "# If necessary, you can also set a threshold, ignoring all improvements below the threshold\n",
    "stopping_callback = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=4,min_delta=0.0001)\n",
    "\n",
    "# NOTE that it may be useful NOT to use early stopping while you are tisll tuning learning convergence\n",
    "# also be sure to adapt the patience in order to allow convergence\n",
    "\n",
    "# Putting verbose = 1 below (instead of 0 i the previous notebook) plots out 1 line per epoch\n",
    "history_1 = model_1.fit(x_train, r_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    callbacks = [cp_callback, stopping_callback],\n",
    "                    validation_data=(x_val, r_val)\n",
    "                    )\n",
    "# If you want to save the weights of this trained model, run:\n",
    "model_1.save_weights(model_savename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 376
    },
    "colab_type": "code",
    "id": "NigJYW4X3Rf9",
    "outputId": "aea9b6a8-4dca-43fb-ea9d-da1ddb1b0a66"
   },
   "outputs": [],
   "source": [
    "# model_1 now contains the model at the end of the training run\n",
    "# We analyse the result:\n",
    "\n",
    "[train_loss, train_accuracy] = model_1.evaluate(x_train, r_train, verbose=0)\n",
    "print(\"Training set Accuracy:{:7.4f}\".format(train_accuracy))\n",
    "print(\"Training set Loss:{:7.6f}\\n\".format(train_loss))\n",
    "\n",
    "[val_loss, val_accuracy] = model_1.evaluate(x_val, r_val, verbose=0)\n",
    "print(\"Validation set Accuracy:{:7.4f}\".format(val_accuracy))\n",
    "print(\"Validation set Loss:{:7.6f}\\n\".format(val_loss))\n",
    "\n",
    "#Now we visualise what happened during training\n",
    "plot_history(history_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-KJQbmv5sxQx"
   },
   "source": [
    "## Reconstructing saved models\n",
    "\n",
    "In the previous code, we have saved models in two ways:\n",
    "\n",
    "- we have saved the weights of the final model\n",
    "- we have saved the complete model for the best validation score that occurred during training\n",
    "\n",
    "In the code below, we show first how to reconstruct a model from saved weights (in this case, the final model) and then how to reconstruct a complete model.\n",
    "\n",
    "For a more extensive explanation of saving and restoring models, we refer to [this notebook](https://colab.research.google.com/drive/1Qwbn5qnayBXo3rkpZvT9dQvLD0UoSCJZ#scrollTo=-KJQbmv5sxQx&line=10&uniqifier=1) from the Tensorflow Docs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 115
    },
    "colab_type": "code",
    "id": "O98ZoaIVpdJj",
    "outputId": "9b68824d-be80-4cd1-ef00-60396e974b2c"
   },
   "outputs": [],
   "source": [
    "# You can later reconstruct this model by first rebuilding a model with the same structure \n",
    "# (so make sure you keep the code for that)\n",
    "# and then loading the weights:\n",
    "\n",
    "reconstructed_model_1 = initial_model() # this is a new model\n",
    "reconstructed_model_1.load_weights(model_savename)\n",
    "\n",
    "# The code below should give the same results as that for the original model\n",
    "[train_loss, train_accuracy] = reconstructed_model_1.evaluate(x_train, r_train, verbose=0)\n",
    "print(\"Training set Accuracy:{:7.4f}\".format(train_accuracy))\n",
    "print(\"Training set Loss:{:7.6f}\\n\".format(train_loss))\n",
    "\n",
    "[val_loss, val_accuracy] = reconstructed_model_1.evaluate(x_val, r_val, verbose=0)\n",
    "print(\"Validation set Accuracy:{:7.4f}\".format(val_accuracy))\n",
    "print(\"Validation set Loss:{:7.6f}\\n\".format(val_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vM6H8LVQybgN"
   },
   "source": [
    "The code above reconstructs your final model. However, if you used early stopping, this is not necessarily the 'best' model, i.e., the one with the highest validation accuracy.\n",
    "\n",
    "We will now reconstruct the best model encountered during training. This was saved by the ModelCheckpoint. Since we told it to 'save_best_only', it has saved model weights each time an improvement occurred. Out best model is therefore the last of teh saved checkpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 181
    },
    "colab_type": "code",
    "id": "vZ_vUZfl3wUR",
    "outputId": "34a7b994-5cf5-43b8-e2d6-e86c1d6e45b4"
   },
   "outputs": [],
   "source": [
    "import pathlib\n",
    "\n",
    "# Find latest checkpoint\n",
    "# Sort the checkpoints by modification time.\n",
    "checkpoints = pathlib.Path(checkpoint_dir).glob(\"*.index\")\n",
    "checkpoints = sorted(checkpoints, key=lambda cp:cp.stat().st_mtime)\n",
    "checkpoints = [cp.with_suffix('') for cp in checkpoints]\n",
    "latest = str(checkpoints[-1])\n",
    "print(latest)\n",
    "checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 115
    },
    "colab_type": "code",
    "id": "w5ymzeG0yw_w",
    "outputId": "9500d860-72cf-453e-ba34-69b08694349b"
   },
   "outputs": [],
   "source": [
    "# Now restore the model\n",
    "\n",
    "best_model_1 = initial_model() # this is a new model\n",
    "best_model_1.load_weights(latest)\n",
    "\n",
    "# The code below should give slightly better results than that for the original model\n",
    "[train_loss, train_accuracy] = best_model_1.evaluate(x_train, r_train, verbose=0)\n",
    "print(\"Training set Accuracy:{:7.4f}\".format(train_accuracy))\n",
    "print(\"Training set Loss:{:7.6f}\\n\".format(train_loss))\n",
    "\n",
    "[val_loss, val_accuracy] = best_model_1.evaluate(x_val, r_val, verbose=0)\n",
    "print(\"Validation set Accuracy:{:7.4f}\".format(val_accuracy))\n",
    "print(\"Validation set Loss:{:7.6f}\\n\".format(val_loss))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WgLBptRm7m4D"
   },
   "source": [
    "## Final model and analysis\n",
    "\n",
    "Once you are happy with how your model performs on the validation set, it is time to re-train it on the original training set (i.e., including the validation set). This means that you no longer have a validation set to use during training. It also means you can not do early stopping. **Do NOT use the test set for this** since this has to remain entirely unused for training!! Instead, you should use the number of EPOCHS that corresponds to your best validation score in the previous runs and fix that. In the run above, the best model was stored after EPOCH 10.\n",
    "\n",
    "Once this re-training has been done, you van analyse how your network performs on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 592
    },
    "colab_type": "code",
    "id": "UQYg4oqj8eaC",
    "outputId": "4de98132-0881-4424-88c7-b792933edeb0"
   },
   "outputs": [],
   "source": [
    "model_for_test = initial_model()\n",
    "model_1.summary()\n",
    "\n",
    "# We now add batch size to the mix of training parameters\n",
    "# If you don't specify batch size below, all training data will be used for each learning step\n",
    "batch_size = 16\n",
    "epochs = 10\n",
    "\n",
    "history_for_test = model_for_test.fit(x_train_all, r_train_all,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1\n",
    "                    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 294
    },
    "colab_type": "code",
    "id": "ZW73OV2u-TxZ",
    "outputId": "fae2341d-0280-4740-b934-04d57f731ae4"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12,4))\n",
    "plt.subplot(1,2,1)\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(history_for_test.epoch, np.array(history_for_test.history['accuracy']),'g-',\n",
    "        label='Train accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss minimised by model')\n",
    "plt.plot(history_for_test.epoch, np.array(history_for_test.history['loss']),'g-',\n",
    "        label='Train loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VT9r4dDu-hk1"
   },
   "source": [
    "Now, we will save the whole model, i.e., both the network structure and the weights, because that makes it easier to analyse it in a separate notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 32
    },
    "colab_type": "code",
    "id": "zkqZvYOE-tCw",
    "outputId": "ee2edfb1-61b7-4cdc-9ac6-81335ba00115"
   },
   "outputs": [],
   "source": [
    "#the file type for storing complete models is \".h5\"\n",
    "# feel free to change the path to whatever suits you best!\n",
    "modelpath = checkpoint_dir+\"final_model.h5\"\n",
    "\n",
    "print(\"Final model saved as \",modelpath)\n",
    "\n",
    "# Save entire model to a HDF5 file\n",
    "model_for_test.save(modelpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PFwYnho5Dja9"
   },
   "source": [
    "Finally, measure the performance of your final model on train set and test set and include those numbers in your report (as well as the performances on train and validation sets from the previous run)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 115
    },
    "colab_type": "code",
    "id": "dbX0o6CjDy0A",
    "outputId": "c9cbfa7e-8596-448c-ee40-6db2b12bc493"
   },
   "outputs": [],
   "source": [
    "# The code below should give the same results as that for the original model\n",
    "[train_loss, train_accuracy] = model_for_test.evaluate(x_train_all, r_train_all, verbose=0)\n",
    "print(\"Training set Accuracy:{:7.4f}\".format(train_accuracy))\n",
    "print(\"Training set Loss:{:7.6f}\\n\".format(train_loss))\n",
    "\n",
    "[test_loss, test_accuracy] = model_for_test.evaluate(x_test, r_test, verbose=0)\n",
    "print(\"Validation set Accuracy:{:7.4f}\".format(test_accuracy))\n",
    "print(\"Validation set Loss:{:7.6f}\\n\".format(test_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NhXlLw0g3RgA"
   },
   "source": [
    "## Now it's your turn\n",
    "This network is just a first attempt. It is clearly overfitting heavily and the accuracy is rather low for such a simple task. Optimising the network is your assignment. You can achieve test accuracies well above 0.98.\n",
    "\n",
    "Make a new notebook in which you use the code examples in this notebook to train your own network. Obviously, you are allowed to add code of your own.\n",
    "Please make sure that the final model notebooks you hand in **only contain necessary code and explanation blocks.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QXv60vtB3RgK"
   },
   "source": [
    "\n",
    "\n",
    "The first step is always to make the network powerful enough: your model should be powerful enough to achieve very high accuracies (very low loss) on the training data, **at least when no regularisation is used**. If that is not the case, it is not adapted to the complexity of the problem. The example network above is definitely not powerful enough!\n",
    "\n",
    "Remember that after each addition (different architecture, addition of regularisation,...), you should check again whether learning still converges and fix this if it doesn't. This means that the curves in the validation plots are sufficiently smooth and that validation accuracy does not improve considerably anymore. To achieve this:\n",
    "\n",
    "*  Tune the batch size: if the batch size is too low, the gradient estimates are not very accurate, again leading to irregular behaviour of the validation curves; the higher the batch size, the better, which is why it is usually advisable to set the batch size to the highest number that still fits in your GPU (memory)\n",
    "* Tune the learning rate: if the learning rate is too high, weight updates get too large and may 'overstep the mark'.You can typically see this as very heavy fluctiations the training and validation curves; you may even see 'exploding gradients'. \n",
    "\n",
    "When learning convergence is achieved and overfitting occurs, add regularisation to reduce overfitting: this should bring training and validation curves closer together; note that there are many regularisation options to explore (L1/L2, setting maximal norms to the weights, adding dropout, using input dropout, ...). Hopefully, this results in a higher validation accuracy.\n",
    "\n",
    "Then, iterate. Note that in deep learning, overfitting is not necessarily a problem, if in the end you get a better validation performance (and if you can live with the variance). If you find in the last step above that overfitting can be completely removed, you should definitely make your network more powerful (again, there are many ways to do this).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Rro4dvE33RgK"
   },
   "source": [
    "When you think you have a good model, purely judged on validation accuracy, retrain it on the whole training set and analyse the final quality on the test set."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "First_graded_assignment_training.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
