{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kyD1qjBA3RfY"
   },
   "source": [
    "# First graded assignment - analysis of model quality\n",
    "\n",
    "In this notebook, we show how to analyse your final trained model's performance on the test set. We will show you how to plot a confusion matrix and how to visualise errors, e.g. to analyse which samples have been misclassified. Especially when you are working with images, it is important to look at (some examples of) your data, as well as at the errors your model makes.\n",
    "\n",
    "Obviously, while tuning your model, you can do the same analyses, but **only on the validation set!**\n",
    "\n",
    "## Setting things up\n",
    "\n",
    "First you need to do the necessary imports again, mount your drive and load the data. Here, we will only need the test set, for evaluation purposes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49
    },
    "colab_type": "code",
    "id": "E8ZDx4K73Rfb",
    "outputId": "64ae91d5-1f07-4670-a754-d6c4629544b3"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "  # %tensorflow_version only exists in Colab.\n",
    "  %tensorflow_version 2.x\n",
    "except Exception:\n",
    "  pass\n",
    "\n",
    "# TensorFlow and tf.keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "print(tf.__version__)\n",
    "\n",
    "# Helper libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn as sk\n",
    "import pandas as pd\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 2020\n",
    "np.random.seed(seed)  \n",
    "\n",
    "import sklearn as sk\n",
    "\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 32
    },
    "colab_type": "code",
    "id": "z4HJ7Y-QACap",
    "outputId": "6ce787b0-d61b-4e0e-9a0e-590853993a01"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')\n",
    "\n",
    "import os\n",
    "\n",
    "#!ls '/content/gdrive/My Drive/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49
    },
    "colab_type": "code",
    "id": "UxP4HNF-3Rfh",
    "outputId": "55c2dd1c-fb58-46d2-86a8-666bff38e80b"
   },
   "outputs": [],
   "source": [
    "# load train and test data\n",
    "(x_train, r_train_class), (x_test, r_test_class) = mnist.load_data()\n",
    "\n",
    "x_test = x_test.reshape(x_test.shape[0], -1)\n",
    "\n",
    "# some preprocessing ... convert integers to floating point and rescale them to [0,1] range\n",
    "x_test = x_test.astype('float32')\n",
    "x_test /= 255\n",
    "\n",
    "print(x_test.shape[0], ' original test samples')\n",
    "\n",
    "# This data set contains a train set and test set\n",
    "# we still need to split off a validation set\n",
    "\n",
    "# Numebr of test samples\n",
    "N_test = x_test.shape[0]\n",
    "\n",
    "r_test = keras.utils.to_categorical(r_test_class)\n",
    "\n",
    "# look at the new labels for the firs sample\n",
    "print(r_test[0])\n",
    "\n",
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 605
    },
    "colab_type": "code",
    "id": "xATelOMr3Rfl",
    "outputId": "1e08ba29-6cec-458c-e616-f7dc9a07373a"
   },
   "outputs": [],
   "source": [
    "# The features in this data set are the pixels of a 28x28 pixel image\n",
    "# You can visualise an individual image as follows\n",
    "# (here for the first 9 images in the test set)\n",
    "\n",
    "f = plt.figure(figsize=(10,10));\n",
    "for idx in range(9):\n",
    "    plt.subplot(3,3,idx+1)\n",
    "    plt.subplots_adjust(hspace=0.5)\n",
    "    plt.title(\"Label is \" + str(r_test_class[idx]))\n",
    "    plt.imshow(np.reshape(x_test[idx,:],(28,28)), cmap='Greys', interpolation='None')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zIqEIVsKAp7N"
   },
   "source": [
    "## Load model and run it on the test data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49
    },
    "colab_type": "code",
    "id": "ZknR9JmzEFkv",
    "outputId": "1dc6b817-b515-49fd-e409-7491638a6e8e"
   },
   "outputs": [],
   "source": [
    "# fill in the path of the model you want to evaluate here\n",
    "modelpath=\"/content/gdrive/My Drive/Colab Notebooks/DL2020/GA1/final_model.h5\"\n",
    "\n",
    "tested_model=tf.keras.models.load_model(modelpath)\n",
    "predictions = tested_model.predict(x_test)\n",
    "\n",
    "# the predictions are class probabilities, \n",
    "# the argmax code line below finds the index of the class with the highest probability\n",
    "\n",
    "test_predicted_class = np.argmax(predictions,axis=1)\n",
    "misclassified = np.where(test_predicted_class!=r_test_class)[0]\n",
    "\n",
    "# misclassified is a vector that contains the indices of all misclassified samples\n",
    "\n",
    "print(\"There are \",misclassified.shape[0],\" misclassified samples in the test set.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hHoLfk9cwYbd"
   },
   "source": [
    "Our scoring measure is accuracy, but it is often useful to look at the performance of the model for individual classes. The measures commonly used in this case are called *precision*, *recall* and *F1-score* (definitions can be found [here](https://en.wikipedia.org/wiki/Confusion_matrix) ).\n",
    "\n",
    "Below, we use an sklearn function to calculate these measures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 715
    },
    "colab_type": "code",
    "id": "RewiMZTtwLgS",
    "outputId": "eea8b13e-530e-42bb-8cbe-988408b32a1a"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "\n",
    "\n",
    "precision, recall, f1, _ = score(r_test_class, test_predicted_class)\n",
    "\n",
    "names = ['0','1','2','3','4','5','6','7','8','9']\n",
    "plt.figure(figsize=(10,12))\n",
    "\n",
    "plt.subplot(3,1,1)\n",
    "plt.bar(names, precision)\n",
    "plt.ylim([0.8,1])\n",
    "plt.title(\"Precision\")\n",
    "plt.subplot(3,1,2)\n",
    "plt.bar(names, recall)\n",
    "plt.title(\"Recall\")\n",
    "plt.ylim([0.8,1])\n",
    "plt.subplot(3,1,3)\n",
    "plt.bar(names, f1)\n",
    "plt.ylim([0.8,1])\n",
    "plt.title(\"F1-score\")\n",
    "plt.show()\n",
    "\n",
    "#print('precision: {}'.format(precision))\n",
    "#print('recall: {}'.format(recall))\n",
    "#print('fscore: {}'.format(f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fdH4FQFPu0FP"
   },
   "source": [
    "## Visualise errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 605
    },
    "colab_type": "code",
    "id": "pyL9nzEgu42J",
    "outputId": "78d0c9ae-4b58-4add-fd42-28f28057d995"
   },
   "outputs": [],
   "source": [
    "print(\"The first 9 misclassified samples look like this:\")\n",
    "\n",
    "f = plt.figure(figsize=(10,10));\n",
    "for idx in range(9):\n",
    "    mis_index = misclassified[idx]\n",
    "    plt.subplot(3,3,idx+1)\n",
    "    plt.subplots_adjust(hspace=0.5)\n",
    "    plt.title(\"Label: \" + str(r_test_class[mis_index]) \n",
    "              + \", prediction: \" + str(test_predicted_class[mis_index]))\n",
    "    plt.imshow(np.reshape(x_test[mis_index,:],(28,28)), cmap='Greys', interpolation='None')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "O8FpgMUZgr1K"
   },
   "source": [
    "We can go further and find some of the misclassified examples about which the model is most certain:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XbfXEfBCqG1_"
   },
   "outputs": [],
   "source": [
    "# the functions below have been borrowed/adapted from the notebook:\n",
    "# https://www.tensorflow.org/tutorials/keras/classification\n",
    "\n",
    "\n",
    "# this function plots the image with some annotations\n",
    "def plot_image(predictions, true_label, img):\n",
    "  plt.grid(False)\n",
    "  plt.xticks([])\n",
    "  plt.yticks([])\n",
    "\n",
    "  plt.imshow(img, cmap=plt.cm.binary)\n",
    "\n",
    "  predicted_label = np.argmax(predictions)\n",
    "  if predicted_label == true_label:\n",
    "    color = 'blue'\n",
    "  else:\n",
    "    color = 'red'\n",
    "\n",
    "  plt.xlabel(\"{} {:2.0f}% ({})\".format(predicted_label,\n",
    "                                100*np.max(predictions),\n",
    "                                true_label),\n",
    "                                color=color)\n",
    "\n",
    "\n",
    "# this function shows the predictions for this sample\n",
    "def plot_value_array(predictions, true_label):\n",
    "  plt.grid(False)\n",
    "  plt.xticks(range(10))\n",
    "  plt.yticks([])\n",
    "  thisplot = plt.bar(range(10), predictions, color=\"#777777\")\n",
    "  plt.ylim([0, 1])\n",
    "  predicted_label = np.argmax(predictions)\n",
    "\n",
    "  thisplot[predicted_label].set_color('red')\n",
    "  thisplot[true_label].set_color('blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 728
    },
    "colab_type": "code",
    "id": "-yD_9mh-g0q0",
    "outputId": "6e415966-987b-4b02-a6be-2cec18af00d6"
   },
   "outputs": [],
   "source": [
    "# probabilities for class that will be predicted\n",
    "maxprob = np.max(predictions, axis=1)\n",
    "\n",
    "# find 20 most certain misclassified samples\n",
    "most_certain = np.argsort(maxprob[misclassified])\n",
    "most_certain = misclassified[most_certain[-20:]]\n",
    "\n",
    "# now we visualise the samples we found\n",
    "\n",
    "# Plot the first X test images, their predicted labels, and the true labels.\n",
    "# Color correct predictions in blue and incorrect predictions in red.\n",
    "num_rows = 5\n",
    "num_cols = 4\n",
    "num_images = num_rows*num_cols\n",
    "plt.figure(figsize=(2*2*num_cols, 2*num_rows))\n",
    "for i in range(num_images):\n",
    "  idx=most_certain[i]\n",
    "  plt.subplot(num_rows, 2*num_cols, 2*i+1)\n",
    "  plot_image(predictions[idx], r_test_class[idx], np.reshape(x_test[idx],(28,28)))\n",
    "  plt.subplot(num_rows, 2*num_cols, 2*i+2)\n",
    "  plot_value_array(predictions[idx], r_test_class[idx])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uLLNNsKFt-DI"
   },
   "source": [
    "## Plotting confusion matrices\n",
    "\n",
    "You can use the function defined below to plot a confusion matrix. This shows how often each class is confused with each other class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nq2-5JUU3RgP"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "\n",
    "def plot_confusion_matrix(true_labels, predictions, \n",
    "                          classes =[], \n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    This code is adapted from \n",
    "    https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "    \"\"\"\n",
    "    cm = confusion_matrix(true_labels, predictions)\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    #print(cm)\n",
    "    \n",
    "    num_classes = max(len(np.unique(true_labels)),len(np.unique(predictions)))\n",
    "    if len(classes) == 0:\n",
    "        num_classes = max(len(np.unique(true_labels)),len(np.unique(predictions)))\n",
    "        classes = range(num_classes)\n",
    "    plt.figure(figsize = (6,6))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 456
    },
    "colab_type": "code",
    "id": "6dT8P2k4b3XD",
    "outputId": "e0401424-a980-45ff-d330-132fdcdcd75b"
   },
   "outputs": [],
   "source": [
    "# An unnormalised confusion matrix shows the numbers of samples in each combination of true class/predicted class\n",
    "\n",
    "plot_confusion_matrix(r_test_class, test_predicted_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 456
    },
    "colab_type": "code",
    "id": "45GGtYX0b5gu",
    "outputId": "59275df5-06a1-4935-c203-7094f37c1897"
   },
   "outputs": [],
   "source": [
    "# A normalised confusion matrix shows for each true class, which fraction of its samples is predicted in each class\n",
    "# since off-diagonal numbers are hopefully small, this is sometimes less informative\n",
    "\n",
    "# for perfectly balances tasks, both should give the same information\n",
    "# for (strongly) unbalanced tasks, they give complementary views\n",
    "\n",
    "plot_confusion_matrix(r_test_class, test_predicted_class, normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SiWzUrXTHTNA"
   },
   "source": [
    "Since the classes are balanced, a normalised and an unnormalised confusion matrix will give you similar information. If they are not, it is useful to analyse both. Because the number of misclassified samples is so small, in this case the unnormalised version will be easier to analyse."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "First_graded_assignment_analysis.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
