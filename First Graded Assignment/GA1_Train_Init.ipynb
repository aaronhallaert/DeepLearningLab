{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GA1 - Train Init.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMNowOIINiuGnHKONWs3fsf"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2aKnWDyp0PT"
      },
      "source": [
        "# First Graded Assignment \r\n",
        "## (Aaron - Architecture 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Us0MnVh_rW0Q"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7ib5c57qAJs"
      },
      "source": [
        "### Init modules and helper functions\r\n",
        "Load modules and set seed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PQ-XU9TLpyHK"
      },
      "source": [
        "try:\r\n",
        "  # %tensorflow_version only exists in Colab.\r\n",
        "  %tensorflow_version 2.x\r\n",
        "except Exception:\r\n",
        "  pass\r\n",
        "\r\n",
        "# TensorFlow and tf.keras\r\n",
        "import tensorflow as tf\r\n",
        "from tensorflow import keras\r\n",
        "\r\n",
        "print(tf.__version__)\r\n",
        "\r\n",
        "# Helper libraries\r\n",
        "import numpy as np\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import sklearn as sk\r\n",
        "import pandas as pd\r\n",
        "\r\n",
        "# fix random seed for reproducibility\r\n",
        "seed = 2021\r\n",
        "np.random.seed(seed)  \r\n",
        "\r\n",
        "import sklearn as sk\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "\r\n",
        "from tensorflow.keras.datasets import mnist\r\n",
        "from tensorflow.keras.models import Sequential\r\n",
        "from tensorflow.keras.layers import Dense, Dropout\r\n",
        "from tensorflow.keras.optimizers import Adam\r\n",
        "from tensorflow.keras.constraints import max_norm\r\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\r\n",
        "from tensorflow.keras.models import load_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gih7AYCrqITl"
      },
      "source": [
        "Helper function to plot history"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rL5nGo4s-PyD"
      },
      "source": [
        "#helper functions for visualisation\n",
        "\n",
        "# same function as in the getting started notebook, \n",
        "# but now plotting the loss functions used in this notebook\n",
        "# we plot the loss we want to optimise on th eleft (in this case: accuracy)\n",
        "def plot_history(history):\n",
        "  plt.figure(figsize = (12,4))\n",
        "  plt.subplot(1,2,1)\n",
        "\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('Accuracy')\n",
        "  plt.plot(history.epoch, np.array(history.history['accuracy']),'g-',\n",
        "           label='Train accuracy')\n",
        "  plt.plot(history.epoch, np.array(history.history['val_accuracy']),'r-',\n",
        "           label = 'Validation accuracy')\n",
        "  plt.legend()\n",
        "\n",
        "  plt.subplot(1,2,2)\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('Loss minimised by model')\n",
        "  plt.plot(history.epoch, np.array(history.history['loss']),'g-',\n",
        "           label='Train loss')\n",
        "  plt.plot(history.epoch, np.array(history.history['val_loss']),'r-',\n",
        "           label = 'Validation loss')\n",
        "  plt.legend()"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "diH1fR8GqPdy"
      },
      "source": [
        "### Mounting Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FkPQKyYsqSkr"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/gdrive')\r\n",
        "\r\n",
        "import os\r\n",
        "\r\n",
        "#!ls '/content/gdrive/My Drive/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bjb18F3cqdYp"
      },
      "source": [
        "### Loading and visualizing the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zuoepw55qh7c"
      },
      "source": [
        "# load train and test data\r\n",
        "(x_train_all, r_train_all_class), (x_test, r_test_class) = mnist.load_data()\r\n",
        "x_train_all = x_train_all.reshape(x_train_all.shape[0], -1)\r\n",
        "x_test = x_test.reshape(x_test.shape[0], -1)\r\n",
        "\r\n",
        "# some preprocessing ... convert integers to floating point and rescale them to [0,1] range\r\n",
        "x_train_all = x_train_all.astype('float32')\r\n",
        "x_test = x_test.astype('float32')\r\n",
        "x_train_all /= 255\r\n",
        "x_test /= 255\r\n",
        "\r\n",
        "print(x_train_all.shape[0], ' original train samples')\r\n",
        "print(x_test.shape[0], ' original test samples')\r\n",
        "\r\n",
        "# This data set contains a train set and test set\r\n",
        "# we still need to split off a validation set\r\n",
        "\r\n",
        "# Number of test samples\r\n",
        "N_test = x_test.shape[0]\r\n",
        "\r\n",
        "# split off 10000 samples for validation\r\n",
        "N_val = 10000\r\n",
        "N_train = x_train_all.shape[0] - N_val\r\n",
        "\r\n",
        "# now extract the samples into train, validate and test sets\r\n",
        "# set random state to make sure you get the same split each time\r\n",
        "x_train, x_val, r_train_class, r_val_class = train_test_split(x_train_all, r_train_all_class, test_size = N_val, random_state=0)\r\n",
        "\r\n",
        "# For initial explorations, it is often useful to \r\n",
        "# try out some things first on a smaller data set\r\n",
        "# in comment below is the code for using all training data\r\n",
        "# in this case, we select 10000 samples for training\r\n",
        "# don't forget to use all training examples for your final model\r\n",
        "# by replacing the line below by the commented one beneath it\r\n",
        "\r\n",
        "N_train = 10000\r\n",
        "\r\n",
        "x_train = x_train[:N_train,:]\r\n",
        "r_train_class = r_train_class[:N_train]\r\n",
        "\r\n",
        "print(\"Using \",x_train.shape[0],\" train samples, \", x_val.shape[0], \" validation samples and \",x_test.shape[0],\" test samples\")\r\n",
        "print(\"Each sample has \",x_train.shape[1],\" features\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tqKL4kOuq2DY"
      },
      "source": [
        "# The features in this data set are the pixels of a 28x28 pixel image\r\n",
        "# You can visualise an individual image as follows\r\n",
        "# (here for the first 9 images in the training set)\r\n",
        "\r\n",
        "f = plt.figure(figsize=(10,10));\r\n",
        "for idx in range(9):\r\n",
        "    plt.subplot(3,3,idx+1)\r\n",
        "    plt.subplots_adjust(hspace=0.5)\r\n",
        "    plt.title(\"Label is \" + str(r_train_class[idx]))\r\n",
        "    plt.imshow(np.reshape(x_train[idx,:],(28,28)), cmap='Greys', interpolation='None')\r\n",
        "\r\n",
        "# The labels are numbers from 0 to 9\r\n",
        "print(\"A few labels:\")\r\n",
        "print(r_train_class[:9])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7JGQyBBrBO5"
      },
      "source": [
        "### Convert labels for multiclass"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c4cdY2CLrHBO"
      },
      "source": [
        "r_train_all = keras.utils.to_categorical(r_train_all_class)\r\n",
        "r_train = keras.utils.to_categorical(r_train_class)\r\n",
        "r_val = keras.utils.to_categorical(r_val_class)\r\n",
        "r_test = keras.utils.to_categorical(r_test_class)\r\n",
        "\r\n",
        "# look at the new labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pOp6909arNvK"
      },
      "source": [
        "## Model Architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LsmJgf1QrS16"
      },
      "source": [
        "num_classes = 10\r\n",
        "\r\n",
        "# this first network has 2 hidden layers\r\n",
        "# the first layer needs to be told explicitly what the input shape is\r\n",
        "# the output layer has 10 neurons: one neuron per class\r\n",
        "\r\n",
        "# Note that we use the \"He\" initialisation scheme here, since this is often advised \r\n",
        "# for layers with ReLu neurons - although you are free to change this, it is not necessary for this assignment\r\n",
        "\r\n",
        "# Also note that \"dropout\" is implemented in separate layers in Keras\r\n",
        "# they are added below in comment to show you how to use them\r\n",
        "# note that you can also start your network with a dropout layer (randomly setting input features to 0)\r\n",
        "\r\n",
        "def initial_model():\r\n",
        "    # create linear model\r\n",
        "    model = Sequential()\r\n",
        "    # we start with a first fully connected layer\r\n",
        "    model.add(Dense(16, activation='relu', input_shape=(784,), kernel_initializer='he_uniform'))\r\n",
        "    ## then add some dropout, set at a very low value for now\r\n",
        "    #model.add(Dropout(0.001))\r\n",
        "    # a second dense layer with half as many neurons\r\n",
        "    model.add(Dense(16, activation='relu', kernel_initializer='he_uniform'))\r\n",
        "    ## some more dropout\r\n",
        "    #model.add(Dropout(0.001))\r\n",
        "    # and the output layer\r\n",
        "    model.add(Dense(num_classes, activation='softmax'))\r\n",
        "\r\n",
        "    model.compile(loss='categorical_crossentropy',\r\n",
        "                  optimizer=Adam(learning_rate=0.001), # set to default learning rate here\r\n",
        "                  metrics=['accuracy']) # save accuracy in addition to cross entropy error\r\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1luGMHMVrkmH"
      },
      "source": [
        "## Training"
      ]
    }
  ]
}